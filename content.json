{"meta":{"title":"Tobey's note","subtitle":null,"description":"Tobey's personal blog","author":"Tobey","url":"http://xiangyu123.github.io"},"pages":[],"posts":[{"title":"ops","slug":"ops","date":"2018-10-17T03:49:05.000Z","updated":"2018-10-17T03:49:21.303Z","comments":true,"path":"2018/10/17/ops/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/ops/","excerpt":"","text":"运维标准化工作.md系统标准规范化 机器电源、网络冗余要求 硬件BIOS配置、RAID初始化(根据物理机的用途，规范初始化硬件设置，提高性能和一致性) 操作系统支持的版本(例如只支持CentOS7/CentOS6) 系统初始化参数(ulimit.conf/sysctl.conf/resolve.conf,yum.repo等) 基础运维组件安装配置（如heka、zabbix-agent等）【重要，不紧急】解决系统配置不统一的问题 应用配置合理化 了解目前线上应用的使用情况及配置 推进配置的改进以提高可靠性和性能解决线上配置保持最优状态的问题 运维流程标准规范 设备上架、下线、资产入库与回收流程 事故处理规范 申请机器的流程规范解决资产管理标准有标准流程的问题 监控系统的完善 关键的基础设施、系统状态、业务状态电视墙展示 处理不合理或者不到位的监控 监控新系统调研或改进解决监控不到位、告警不及时、展示没重点、告警频繁没聚合的问题 容量规划平台 容量规范方案 搭建自动化压测环境 开发容量规范平台 将压测数据注入压测平台生成性能指标数据解决线上容量及扩展不清楚、资产采购拍脑袋的问题 数据备份 统一规划、管理、分配用于备份数据的设备和空间。 跟维护人员及业务部门确认数据的备份要求。 运维相关平台及配置的备份 分配资源给相关的负责人，并跟进相关负责人的业务数据备份情况。解决备份空间混用关键业务数据可能关键时候找不到备份的问题 日志问题规范化 推进业务日志的轮转及保存路径统一 推进业务日志的收集客户端版本及配置合理解决目前主机磁盘空间、CPU负载高、测试看不到日志需要上线过程中调试日志效率低的问题 故障管理及应急预案 故障定位和快速恢复 业务的架构和逻辑处理深入学习 故障定位的及时响应，快速定位故障点，科学恢复 对故障的等级进行评估和优化改进 排查故障带来二次故障灾害 定期进行应急演练解决定位故障慢，业务架构不熟悉的问题 代码review","categories":[],"tags":[{"name":"ops","slug":"ops","permalink":"http://xiangyu123.github.io/tags/ops/"}]},{"title":"k8s-install","slug":"k8s-install","date":"2018-10-17T03:34:27.000Z","updated":"2018-10-17T03:52:37.046Z","comments":true,"path":"2018/10/17/k8s-install/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/k8s-install/","excerpt":"","text":"1. 环境准备 机器名 资源配置 操作系统 角色 IP k8s101 2/cpu+2G/mem CentOS7.4-x86_64 Master 172.17.8.101 k8s102 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.102 k8s103 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.103 2. 应用版本准备 docker版本 etcd版本 CoreDNS版本 kubernetes版本 flannel版本 docker-ce-18.03.1.ce-1.el7 etcd-v3.1.12 v1.1.3 v1.10.5 v0.10.0 3. 机器角色的分配 k8s101- kube-apiserver - kube-controller-manager - kube-scheduler - etcd k8s102- kubelet - kube-proxy - docker - flannel - etcd k8s103- kubelet - kube-proxy - docker - flannel - etcd 4. 所有机器安装集群依赖的软件包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#清空防火墙规则iptables -t nat -F &amp;&amp; iptables -t nat -X &amp;&amp; iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -Z &amp;&amp; iptables -t nat -Z#k8s 1.8+要求关闭swapswapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab#修改内核参数cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1EOFsysctl -p#kubernetes 1.8+要求关闭swap分区(master节点，建议node节点也关闭swap)swapoff -a &amp;&amp; sysctl -w vm.swappiness=0#创建k8s组件安装目录mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl&#125;#配置selinuxsed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/configsetenforce 0#安装依赖包yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker官方yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo#安装docker-ceyum install docker-ce#启动dockersystemctl start docker#配置docker加速和私有仓库cat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"192.168.0.210:5000\"] #私有registry的地址&#125;EOF#删除上面文件中的\"私有registry注释\"sed -i 's/#.*//g' /etc/docker/daemon.json#重启docker服务systemctl restart docker 5.1 准备自签名证书(其中一台机器操作即可) 组件 使用到的证书 etcd ca.pem, kubernetes-key.pem, kubernetes.pem flannel ca.pem, kubernetes-key.pem, kubernetes.pem kube-apiserver ca.pem, kubernetes-key.pem, kubernetes.pem kubelet ca.pem kube-proxy ca.pem, kube-proxy.pem, kube-proxy-key.pem kubectl ca.pem, admin.pem, admin-key.pem kube-controller 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 kube-schedule 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 123456789101112131415#安装证书生成个工具wget -c https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl*mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo#创建一个证书的目录sslmkdir sslcd ssl#生成证书模板cfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json 5.2 修改刚生成的json文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110根据config.json创建ca-config.json:&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;ca-csr.json:#需要注意的是CN,O和OU的属性都不要修改&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;kubernetes-csr.json:&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"etcd101\", \"etcd102\", \"etcd103\", \"127.0.0.1\", \"172.17.8.101\", \"172.17.8.102\", \"172.17.8.103\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;admin-csr.json:&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;kube-proxy-csr.json&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; 5.3 生成相应的证书和key1234567891011#生成ca的证书和keycfssl gencert -initca ca-csr.json | cfssljson -bare ca#生成服务器证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes#生成admin的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin#生成kube-proxy的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 5.4 查看一下是否生成了相应的证书和key123456789101112#仅保留相应的pem证书mkdir ../certmv *.pem ../cert#校验证书cfssl-certinfo -cert kubernetes.pem#查看相应的证书是否生成[root@k8s101 ssl]# cd ../cert/[root@k8s101 cert]# lsadmin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem[root@k8s101 cert]# 6.1 安装etcd组件(所有机器)12345678#下载etcdwget -c https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz#解压安装tar xf etcd-v3.1.12-linux-amd64.tar.gzcd etcd-v3.1.12-linux-amd64mv etcd* /opt/kubernetes/bin/mkdir /var/lib/etcd 6.2 分别配置每个机器的参数(k8s101为例)1234567891011121314cat &gt; /opt/kubernetes/cfg/etcd &lt;&lt; EOF#[Member]ETCD_NAME=\"etcd101\"ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd/\"ETCD_LISTEN_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.17.8.101:2379,http://127.0.0.1:2379\"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.17.8.101:2379\"ETCD_INITIAL_CLUSTER=\"etcd101=https://172.17.8.101:2380,etcd102=https://172.17.8.102:2380,etcd103=https://172.17.8.103:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF 6.3 配置etcd的服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#把下面这个粘贴到/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetwants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/opt/kubernetes/cfg/etcd#User=etcd# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) \\/opt/kubernetes/bin/etcd \\--name=\\\"$&#123;ETCD_NAME&#125;\\\" \\--data-dir=\\\"$&#123;ETCD_DATA_DIR&#125;\\\" \\--listen-peer-urls=\\\"$&#123;ETCD_LISTEN_PEER_URLS&#125;\\\" \\--listen-client-urls=\\\"$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\\" \\--advertise-client-urls=\\\"$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\\\" \\--initial-advertise-peer-urls=\\\"$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125;\\\" \\--initial-cluster=\\\"$&#123;ETCD_INITIAL_CLUSTER&#125;\\\" \\--initial-cluster-token=\\\"$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\\\" \\--initial-cluster-state=\\\"$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\\\" \\--cert-file=/opt/kubernetes/ssl/kubernetes.pem\\--key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--peer-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\--peer-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\" \\--client-cert-auth=\"\\true\\\" \\--peer-client-cert-auth=\\\"true\\\" \\--auto-tls=\\\"true\\\" \\--peer-auto-tls=\\\"true\\\"\"Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target#使得service生效systemctl daemon-reload#把需要用到的证书拷贝到目的地cp ca.pem /opt/kubernetes/ssl/cp server.pem /opt/kubernetes/ssl/cp server-key.pem /opt/kubernetes/ssl/#启动服务systemctl start etcd#验证服务/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 cluster-health#也可以这样验证#[root@k8s103 ssl]# curl -s --cacert /opt/kubernetes/ssl/ca.pem https://172.17.8.101:2379/v2/members | jq .[root@k8s103 ssl]# curl -s http://127.0.0.1:2379/v2/members |jq .&#123; \"members\": [ &#123; \"id\": \"52a549447e771f7\", \"name\": \"etcd102\", \"peerURLs\": [ \"https://172.17.8.102:2380\" ], \"clientURLs\": [ \"https://172.17.8.102:2379\" ] &#125;, &#123; \"id\": \"680948716edebf39\", \"name\": \"etcd103\", \"peerURLs\": [ \"https://172.17.8.103:2380\" ], \"clientURLs\": [ \"https://172.17.8.103:2379\" ] &#125;, &#123; \"id\": \"bf65263f1f4624b4\", \"name\": \"etcd101\", \"peerURLs\": [ \"https://172.17.8.101:2380\" ], \"clientURLs\": [ \"https://172.17.8.101:2379\" ] &#125; ]&#125; 7. 下载flannel并安装应用到所有Node节点,假设k8s102123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#下载flannel-v0.10wget -c https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz#解压tar xf flannel-v0.10.0-linux-amd64.tar.gz#添加flannel的配置文件/opt/kubernetes/cfg/flanneldFLANNEL_OPTIONS=\"--etcd-endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 -etcd-cafile=/opt/kubernetes/ssl/ca.pem -etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"#创建一个供flanneld工作的目录mkdir -p /opt/kubernetes/run/flanneld/#把下面这段粘贴到/usr/lib/systemd/system/flanneld.service[Unit]Description=Flannel overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld -iface=eth1 --ip-masq $FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /opt/kubernetes/run/flanneld/subnet.envRestart=on-failure[Install]wantedBy=multi-user.target#第一个节点启动flanneld之前需要先设置vxlan网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 set /coreos.com/network/config '&#123;\"Network\": \"172.20.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'#尽量使用172开头的网段#查看网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 get /coreos.com/network/config#查看生成的网段信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 ls /coreos.com/network/subnets#修改docker的服务，#在[Service]中ExecStart上面添加一行为EnvironmentFile=/opt/kubernetes/run/flanneld/subnet.env#下面的ExecStart修改为ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS#重启docker服务systemctl daemon-reloadsystemctl restart docker#此时，每个节点分配的docker0的网段应该都是不一样的，ping一下另外一个节点docker0的地址测试 8.1 生成k8s TLS Boostrapping Token(在任意一个master上操作)12345mkdir kubeconfig &amp;&amp; cd kubeconfigexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 8.2 创建kubelet bootstrapping kubeconfig文件123456789101112131415161718192021222324252627282930313233#安装kubectl客户端工具wget -c https://dl.k8s.io/v1.10.5/kubernetes-client-linux-amd64.tar.gztar xf kubernetes-client-linux-amd64.tar.gzcd kubernetes/clientmv kubectl /opt/kubernetes/bin/export KUBE_APISERVER=\"https://172.17.8.101:6443\"#添加环境变量vi ~/.bash_profilePATH修改为 PATH=$PATH:$HOME/bin:/opt/kubernetes/binsource ~/.bash_profile#创建kubectl bootstrapping kubeconfig文件# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 8.3 创建kube-proxy bootstrapping kubeconfig文件123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 9.把刚才创建的kubeconfig文件拷贝到所有的node节点上，位置先随意1234scp kube-proxy.kubeconfig node1_ip:~/...scp bootstrap.kubeconfig node1_ip:~/... 10. 安装master节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158rm -rf kuberneteswget -c https://dl.k8s.io/v1.10.5/kubernetes-server-linux-amd64.tar.gztar xf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bin#把相应的二进制文件放到指定位置mv kube-apiserver /opt/kubernetes/bin/mv kube-controller-manager kube-scheduler /opt/kubernetes/bin/#把下面的内容保存为apiserver.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"192.168.1.195\"&#125;ETCD_SERVERS=$&#123;2:-\"http://127.0.0.1:2379\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--etcd-servers=$&#123;ETCD_SERVERS&#125; \\\\--insecure-bind-address=127.0.0.1 \\\\--bind-address=$&#123;MASTER_ADDRESS&#125; \\\\--insecure-port=8080 \\\\--secure-port=6443 \\\\--advertise-address=$&#123;MASTER_ADDRESS&#125; \\\\--allow-privileged=true \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\\\--kubelet-https=true \\\\--enable-bootstrap-token-auth \\\\--token-auth-file=/opt/kubernetes/cfg/token.csv \\\\--service-node-port-range=30000-50000 \\\\--tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\\\--tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\\\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--etcd-cafile=/opt/kubernetes/ssl/ca.pem \\\\--etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\\\--etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-apiserversystemctl restart kube-apiserver#执行apiserver.sh./apiserver.sh 172.17.8.101 https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379#拷贝刚才生成的tokencd ~/kubeconfigcp token.csv /opt/kubernetes/cfg/#启动kube-apiserversystemctl start kube-apiserver#把下面的内容保存为controller-manager.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect=true \\\\--address=127.0.0.1 \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--cluster-name=kubernetes \\\\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--root-ca-file=/opt/kubernetes/ssl/ca.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-controller-managersystemctl restart kube-controller-manager#安装和启动controller-manager./controller-manager.sh 127.0.0.1#验证controller-manager是否启动systemctl status kube-controller-manager#把下面的内容保存为scheduler.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-schedulersystemctl restart kube-scheduler#安装并启动kube-scheduler./scheduler.sh 127.0.0.1#查看是否启动systemctl status kube-scheduler#验证服务是否都正常kubectl get cs#输入如下NAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-1 Healthy &#123;\"health\": \"true\"&#125;#至此master配置完毕kubectl get svc#如果要修改网段的话，可以修改api-server和controller-manager相应的配置文件，重启服务然后删除kubernetes服务即可#kubectl delete svc default 11. 安装配置node 建议kubelet参数修改加上KUBELET_EXTRA_ARGS= “–fail-swap-on=false” 忽略swap的配置 kubelet v1.11.1 建议配置 KUBE_PROXY_MODE=ipvs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112#在master上执行，创建角色绑定kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap#下面都是在node节点上操作#把刚才传过来的kubeconfig文件都放在指定位置cd kubeconfig/cp *kubeconfig /opt/kubernetes/cfg/#下载client包wget -c https://dl.k8s.io/v1.10.5/kubernetes-node-linux-amd64.tar.gztar xf kubernetes-node-linux-amd64.tar.gzcd kubernetes/node/bin/mv kubelet kube-proxy /opt/kubernetes/bin/#把下面的内容保存为kubelet.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.196\"&#125;DNS_SERVER_IP=$&#123;2:-\"10.10.10.2\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kubeletKUBELET_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--address=$&#123;NODE_ADDRESS&#125; \\\\--hostname-override=$&#123;NODE_ADDRESS&#125; \\\\--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\\--experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\\--cert-dir=/opt/kubernetes/ssl \\\\--allow-privileged=true \\\\--cluster-dns=$&#123;DNS_SERVER_IP&#125; \\\\--cluster-domain=cluster.local \\\\--fail-swap-on=false \\\\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/kubeletExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet#执行脚本bash kubelet.sh 172.17.8.102 10.254.0.2#把下面的内容保存到proxy.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.200\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=$&#123;NODE_ADDRESS&#125; \\--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-proxysystemctl restart kube-proxy#执行proxy.shbash proxy.sh 172.17.8.102#然后在master节点上操作,看到是Pending状态kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 2m kubelet-bootstrap Pending#这时候在master上授权允许就可以了kubectl certificate approve node-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4#再次查看状态就变过来了kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 4m kubelet-bootstrap Approved,Issued#查看节点kubectl get nodesNAME STATUS ROLES AGE VERSION172.17.8.102 Ready &lt;none&gt; 1m v1.10.5#其他的客户端跟这个一样操作即可，等node节点状态变为Ready就可用了#测试一个实例kubectl run nginx --image=nginx --replicas=3kubectl scale --replicas=4 deployment/nginxkubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort 12. 配置一个客户端kubectl123456789101112131415#把admin.pem和admin-key.pem,ca.pem拷贝到你想配置的节点上#在这个节点上下载kubectlkubectl config set-cluster kubernetes --server=https://172.17.8.101:6443 --certificate-authority=ca.pemkubectl config set-credentials cluster-admin --certificate-authority=ca.pem --client-key=admin-key.pem --client-certificate=admin.pemkubectl config set-context default --cluster=kubernetes --user=cluster-adminkubectl config use-context default#测试kubectl get cskubectl get csr#其实会自动生成配置文件，路径为 ~/.kube/config 13.1 配置Dashboard相关yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#把下面内容保存为dashboard-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kubernetes-dashboard-minimal namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system#把下面内容保存为dashboard-deployment.yamlapiVersion: apps/v1beta2kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: kubernetes-dashboard containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.7.1 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 100Mi ports: - containerPort: 9090 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\"#把下面的内容保存为dashboard-service.yamlapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090 13.2 安装dashboard12345678910111213141516kubectl create -f dashboard-rbac.yamlkubectl create -f dashboard-deployment.yamlkubectl create -f dashboard-service.yaml#查看并测试kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.254.116.162 &lt;none&gt; 80:31523/TCP 1d#看到了31523了吧，执行下面的命令kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkubernetes-dashboard-b9f5f9d87-jszl5 1/1 Running 0 1d 172.68.24.2 172.17.8.102#看到了dashboard在172.17.8.102这个node上了吧#访问http://172.17.8.102:31523 就可以打开这个dashboard 14.1 把下面内容保存为coredns.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156apiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/16 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\"spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" containers: - name: coredns image: index.tenxcloud.com/xiangyu123/coredns:1.1.3 imagePullPolicy: IfNotPresent args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\"spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP 14.2 安装过程1kubectl create -f coredns.yaml","categories":[],"tags":[{"name":"k8s, kubernetes","slug":"k8s-kubernetes","permalink":"http://xiangyu123.github.io/tags/k8s-kubernetes/"}]}]}