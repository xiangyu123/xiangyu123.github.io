{"meta":{"title":"Tobey's note","subtitle":null,"description":"Tobey's personal blog","author":"Tobey","url":"http://xiangyu123.github.io"},"pages":[],"posts":[{"title":"ldap","slug":"ldap和kerberos安装","date":"2018-10-17T10:22:22.000Z","updated":"2018-10-17T10:37:53.001Z","comments":true,"path":"2018/10/17/ldap和kerberos安装/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/ldap和kerberos安装/","excerpt":"","text":"OpenLdap搭建 LDAP是一款轻量级目录访问协议（Lightweight Directory Access Protocol，简称LDAP），属于开源集中账号管理架构的实现 1、OpenLdap简介&nbsp;&nbsp;&nbsp;&nbsp;OpenLDAP目录中的信息是以树状的层次结构来存储数据(（这很类同于DNS），最顶 层即根部称作“基准DN”，形如“dc=mydomain,dc=org”或者“o=mydomain.org”，前一种方式更为灵活也是Windows AD中使用的方式。 &nbsp;&nbsp;&nbsp;&nbsp;在根目录的下面有很多的文件和目录，为了把这些大量的数据从逻辑上分开，OpenLDAP像其它的目录服务协议一样使用OU（Organization Unit，组织单元），可以用来表示公司内部机构，如部门等，也可以用来表示设备、人员等。同时OU还可以有子OU，用来表示更为细致的分类。 &nbsp;&nbsp;&nbsp;&nbsp;OpenLDAP中每一条记录都有一个唯一的区别于其它记录的名字DN（Distinguished Name）,其处在“叶子”位置的部分称作RDN(用户条目的相对标识名)。如dn:cn=tom,ou=animals,dc=ilanni,dc=com中cn即为RDN，而RDN在一个OU中必须是唯一的。 其中o=dlw.com是dc=dlw,dc=com的简写 部署的组织架构:12345dn: dc=qbos,dc=com # 公司dn: ou=krb5,dc=qbos,dc=com # 部门dn: cn=admin,dc=qbos,dc=com # LDAP管理用户dn: cn=kdc-srv,ou=krb5,dc=qbos,dc=com # kdc连接ldap用户dn: cn=adm-srv,ou=krb5,dc=qbos,dc=com # kadmin连接ldap用户 1、安装软件包1# yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y 2、初始化配置12# cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG 生产ldap服务管理用户密码12# slappasswd -s 123123&#123;SSHA&#125;FGTQwm900Ac+FPPTpzB9R1ZHNVY/MOXP # hash加密后的值，直接复制到配置文件中使用 修改slap.conf，支持kerberos123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# vim /etc/openldap/slapd.confinclude /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/nis.schema# kerberos使用的schema文件include /etc/openldap/schema/kerberos.schemapidfile /var/run/openldap/slapd.pidargsfile /var/run/openldap/slapd.argsdatabase bdb# 组织后缀suffix &quot;dc=qbos,dc=com&quot; # 管理员账号rootdn &quot;cn=admin,dc=qbos,dc=com&quot;# 管理员密码(slappasswd生产的密码)rootpw &#123;SSHA&#125;FslGWaaPsw9XcM7Sr3gc24lFQQe1NxRA directory /var/lib/ldap# ldap数据库索引字段index krbPrincipalName eq,pres,sub index objectClass eq,presloglevel 256# acl规则access to attrs=userPassword,shadowLastChange by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by anonymous auth by self write by * noneaccess to dn.subtree=&quot;ou=krb5,dc=qbos,dc=com&quot; by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write # kerberos服务连接ladp的账号 by dn=&quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot; write # kerberos服务连接ladp的账号 by dn=&quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot; read by * noneaccess to attrs=loginShell by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by self write by * noneaccess to dn.base=&quot;&quot; by * readaccess to * by self write by * readaccess to * by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by users read by * noneaccess to * by dn.exact=&quot;cn=admin,dc=qbos,dc=com&quot; read by * none 3、初始化ldap配置&nbsp;&nbsp;&nbsp;&nbsp;官方 对于 OpenLDAP 2.4 ，不推荐使用 slapd.conf 作为配置文件。从这个版本开始所有配置数据都保存在 /etc/openldap/slapd.d/中,在slapd.d生成配置文件，但是还是能使用slapd.conf修改配置的，每次修改后都要执行slaptest -f 命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# rm -rf /etc/openldap/slapd.d/*修改文件权限# chown ldap.ldap -R /var/lib/ldap/每次修改完sldp.conf配置文件都要删除重新执行# slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d/报错忽略&gt; 5b99e55c bdb_db_open: database &quot;dc=qbos,dc=com&quot;: db_open(/var/lib/ldap/id2entry.bdb) failed: No such file or directory (2).&gt; 5b99e55c backend_startup_one (type=bdb, suffix=&quot;dc=qbos,dc=com&quot;): bi_db_open failed! (2)&gt; slap_startup failed (test would succeed using the -u switch)# chown ldap.ldap -R /etc/openldap/slapd.d/启动sldap服务# /etc/init.d/slapd start初始化ldap库，编写kerberos.ldif文件# vim /tmp/kerberos.ldifdn: dc=qbos, dc=comdc: qbosobjectClass: topobjectClass: dcObjectobjectClass: organizationo: qbos.comdn: ou=krb5, dc=qbos,dc=comou: krb5objectClass: organizationalUnitdn: cn=kdc-srv,ou=krb5,dc=qbos,dc=comcn: kdc-srvobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: &#123;SSHA&#125;o4pwTcWIoY/CX8mx7gUyy0PRdwfKilAx dn: cn=adm-srv,ou=krb5,dc=qbos,dc=comcn: adm-srvobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: &#123;SSHA&#125;o4pwTcWIoY/CX8mx7gUyy0PRdwfKilAx注：userPassword中的密码是slappasswd生成的导入kerberos.ldif到ldap库中# ldapadd -x -D &apos;cn=admin,dc=qbos,dc=com&apos; -w 123123 -h 127.0.0.1 -f /tmp/kerberos.ldif注：userPassword中的密码是slappasswd生成的, -w 是&apos;cn=admin,dc=qbos,dc=com&apos;真实的用户密码查看ldap数据库信息# ldapsearch -x -D &apos;cn=admin,dc=qbos,dc=com&apos; -w 123123 -b &apos;cn=adm-srv,ou=krb5,dc=qbos,dc=com&apos; 搭建kerberos服务端 Kerberos 服务是单点登录系统，这意味着您对于每个会话只需向服务进行一次自我验证，即可自动保护该会话过程中所有后续事务的安全。服务对您进行验证后，即无需在每次使用基于 Kerberos 的服务时进行验证 官方文档 1、安装过程1# yum install krb5-server* krb5-auth-dialog* krb5-libs* krb5-workstation* -y 2、修改配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162编辑krb5.conf# vim /etc/krb5.conf[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = QBOS.COM # realm要与ldap DC保持一致 dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h # 票据生命周期 renew_lifetime = 7d forwardable = false[realms] QBOS.COM = &#123; kdc = 127.0.0.1:88 # kdc 服务 kadmin_server = 127.0.0.1:749 # 远程管理kdc服务 database_module = openldap_ldapconf # 数据信息保存 &#125;[domain_realm] .qbos.com = QBOS.COM qbos.com = QBOS.COM[kdc] profile = /var/kerberos/krb5kdc/kdc.conf # 密钥分发中心配置文件[dbdefaults] ldap_kerberos_container_dn = ou=krb5,dc=qbos,dc=com[dbmodules] openldap_ldapconf = &#123; db_library = kldap ldap_kdc_dn = &quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot; # kdc-srv访问ldap用户 ldap_kadmind_dn = &quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot; # adm-srv kadmin访问ldap用户 ldap_service_password_file = /var/kerberos/krb5kdc/service.keyfile # 用户密码文件,使用kdb5_ldap_util生成 ldap_servers = ldap://127.0.0.1 # ldap服务 ldap_conns_per_server = 5&#125;# mkdir /var/kerberos/krb5kdc/ssl/# vim /var/kerberos/krb5kdc/kdc.conf[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] QBOS.COM = &#123; pkinit_identity = FILE:/var/kerberos/krb5kdc/ssl/kdc.pem,/var/kerberos/krb5kdc/ssl/kdckey.pem # kdc服务X.509格式的证书 pkinit_anchors = FILE:/var/kerberos/krb5kdc/ssl/cacert.pem # 指定KDC信任签署客户端证书的可信根证书(CA)的位置 acl_file = /var/kerberos/krb5kdc/kadm5.acl # 文件应包含所有获许管理 KDC 的主体名称 dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab # kadmind主体的Keytab ：kadmin / fqdn，changepw / fqdn和kadmin / changepw supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal &#125; 配置文件详解: https://docs.oracle.com/cd/E86824_01/html/E54775/kdc.conf-4.html 如果使用pkinit配置，生产principal时要添加 +requires_preauth 向数据库添加管理主体可以根据需要添加任意数目的 admin 主体。至少必须添加一个 admin 主体，这样才能完成 KDC 配置过程1234# kadmin.local: addprinc kadmin/adminEnter password for principal kadmin/admin@QBOS.COM:&lt;Type the password&gt;Re-enter password for principal kadmin/admin@QBOS.COM: &lt;Type it again&gt;Principal &quot;kadmin/admin@QBOS.COM&quot; created. 为 kadmind 服务创建一个密钥表文件此命令序列创建一个包含 kadmin/ 和 changepw/ 的主体项的特殊密钥表文件。kadmind 服务需要使用这些主体，要更改口令也需要使用这些主体1234# kadmin.local: ktadd -k /var/kerberos/krb5kdc/kadm5.keytab kadmin/admin@QBOS.COM# kadmin.local: ktadd -k /var/kerberos/krb5kdc/kadm5.keytab kadmin/changepw@QBOS.COM参考文件：https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html 3、生成访问ldap的服务密码文件1234# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -w 123123 stashsrvpw -f /var/kerberos/krb5kdc/service.keyfile &quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot;# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -w 123123 stashsrvpw -f /var/kerberos/krb5kdc/service.keyfile &quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot;注：cn=admin,dc=qbos,dc=com 为ldap管理员与密码 4、创建kerberos数据库123# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -H ldap:// create -r QBOS.COM 注：输入cn=admin,dc=qbos,dc=com密码以及创建KDC database master key密码 5、启动kerberos服务123456789# /etc/init.d/krb5kdc start# /etc/init.d/kadmin start# netstat -tnlp|egrep &apos;kadmin|krb5kdc&apos;tcp 0 0 0.0.0.0:749 0.0.0.0:* LISTEN 11829/kadmind tcp 0 0 0.0.0.0:464 0.0.0.0:* LISTEN 11829/kadmind tcp 0 0 0.0.0.0:88 0.0.0.0:* LISTEN 11454/krb5kdc tcp 0 0 :::749 :::* LISTEN 11829/kadmind tcp 0 0 :::464 :::* LISTEN 11829/kadmind tcp 0 0 :::88 :::* LISTEN 11454/krb5kdc 6、测试创建kerberos用户1234567891011121314151617181920212223242526# kadmin.local&gt; Authenticating as principal root/admin@QBOS.COM with password.&gt; kadmin.local: addprinc admintest&gt; WARNING: no policy specified for admintest@QBOS.COM; defaulting to no policy&gt; Enter password for principal &quot;admintest@QBOS.COM&quot;: &gt; Re-enter password for principal &quot;admintest@QBOS.COM&quot;: &gt; Principal &quot;admintest@QBOS.COM&quot; created.&gt; kadmin.local: q查看创建的用户是否写入ldap库中# slapcat |grep admintest5b9a34c3 bdb_monitor_db_open: monitoring disabled; configure monitor database to enabledn: krbPrincipalName=admintest@QBOS.COM,cn=QBOS.COM,ou=krb5,dc=qbos,dc=comkrbPrincipalName: admintest@QBOS.COM验证用户登录kinit admintestPassword for admintest@QBOS.COM: # klist Ticket cache: FILE:/tmp/krb5cc_0Default principal: admintest@QBOS.COMValid starting Expires Service principal09/13/18 17:58:46 09/14/18 17:58:46 krbtgt/QBOS.COM@QBOS.COM renew until 09/13/18 17:58:46 7、问题12345krb5kdc: Can not fetch master key (error: ?????????). - while fetching master key K/M for realm QBOS.COM原因：未初始化kerberos数据库解决：kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -H ldap:// create -r QBOS.COM (数据库为LDAP)如果不是LDAP，使用 /usr/sbin/kdb5_util create -s 搭建kerberos客户端1、安装kerberos客户端1# yum install -y krb5-libs krb5-workstation krb5-devel 2、修改配置文件123456789101112131415161718192021222324252627282930313233修改kerberos配置文件# vim /etc/krb5.conf[libdefaults] default_realm = QBOS.COM #默认域 dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = false[realms] QBOS.COM = &#123; kdc = kdc1.qbos.com:88 # kdc配置 kdc = kdc2.qbos.com:88 kdc = kdc3.qbos.com:88 admin_server = kadmin.qbos.com:749 # kadmin配置 &#125;[domain_realm] .qbos.com = QBOS.COM qbos.com = QBOS.COM 修改sshd_conf,使其支持kerberos认证# vim /etc/ssh/sshd_config添加GSSAPIAuthentication yesGSSAPICleanupCredentials yesKerberosAuthentication yesKerberosOrLocalPasswd yesKerberosTicketCleanup yes# /etc/init.d/sshd restart 3、生成keytab密钥表 &nbsp;&nbsp;&nbsp;&nbsp;keytab密钥表是可以免密登录，如果用户有独立的密钥表是可以免密登录,提供服务的每台主机都必须包含称为 keytab（密钥表）的本地文件。 &nbsp;&nbsp;&nbsp;&nbsp; 要将服务密钥添加至密钥表文件，应使用 kadmin 的 ktadd 命令，将相应的服务主体添加至主机的密钥表文件。由于要将服务主体添加至密钥表文件，因此该主体必须已存在于 Kerberos 数据库中，以便 kadmin 可验证其存在创建主机密钥表 12345678创建主机kerberos主体# kadmin.local -p kadmin/admin -q &quot;addprinc -policy host_policy -randkey host/$HOSTNAME@QBOS.COM&quot;创建主机keytab密钥表# kadmin.local -p kadmin/admin -q &quot;ktadd -k /root/krb5.keytab.$IP host/$HOSTNAME@QBOS.COM&quot;将主机的keytab密钥表拷贝到客户端/etc/# scp /root/krb5.keytab.$IP $HOSTNAME:/etc/ 4、kerberos用户授权主机用户配置文件： .k5login - 用于主机访问的Kerberos V5 acl文件，该文件驻留在用户的主目录，包含了Kerberos主体列表12345在客户端主机172.28.0.106的root家目录下修改.k5login，授权给用户admintest# vim ~/.k5login...admintest@QBOS.COM admintest通过kinit获取到票据后就可以ssh root@172.28.0.106 注：-policy为指定密码安全策略，可以使用 list_policies 查看密码策略列表，使用get_policy查看策略详情 kerberos双活与主从kerberos 双主+主从实际上是通知ldap实现的，只需实现LDAP的双主+主从 1、LDAP双主+主从部署LDAP双主配置文档LDAP主从配置文档 2、kerberos服务端配置按照 搭建kerberos客户端 安装kerberos客户端，将 另一台 kerberos服务端配置CA、key、.k5.QBOS.COM 拷贝至相应位置即可 参考文档：https://www.ilanni.com/?p=13775https://www.cnblogs.com/zhaojonjon/p/5967281.htmlhttps://www.cnblogs.com/zihanxing/p/7001201.htmlhttps://docs.oracle.com/cd/E26926_01/html/E25889/refer-15.htmlhttp://k5wiki.kerberos.org/wiki/LDAP_on_Kerberos kerberos主从https://help.ubuntu.com/lts/serverguide/kerberos-ldap.html.en kerberos x.509证书https://web.mit.edu/kerberos/krb5-latest/doc/admin/pkinit.html ssh conf配置https://www.cnblogs.com/Rozdy/p/4642928.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xiangyu123.github.io/categories/Linux/"}],"tags":[{"name":"ldap","slug":"ldap","permalink":"http://xiangyu123.github.io/tags/ldap/"}]},{"title":"k8s-install","slug":"k8s-install","date":"2018-10-17T03:34:27.000Z","updated":"2018-10-17T10:25:15.000Z","comments":true,"path":"2018/10/17/k8s-install/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/k8s-install/","excerpt":"","text":"1. 环境准备 机器名 资源配置 操作系统 角色 IP k8s101 2/cpu+2G/mem CentOS7.4-x86_64 Master 172.17.8.101 k8s102 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.102 k8s103 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.103 2. 应用版本准备 docker版本 etcd版本 CoreDNS版本 kubernetes版本 flannel版本 docker-ce-18.03.1.ce-1.el7 etcd-v3.1.12 v1.1.3 v1.10.5 v0.10.0 3. 机器角色的分配 k8s101- kube-apiserver - kube-controller-manager - kube-scheduler - etcd k8s102- kubelet - kube-proxy - docker - flannel - etcd k8s103- kubelet - kube-proxy - docker - flannel - etcd 4. 所有机器安装集群依赖的软件包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#清空防火墙规则iptables -t nat -F &amp;&amp; iptables -t nat -X &amp;&amp; iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -Z &amp;&amp; iptables -t nat -Z#k8s 1.8+要求关闭swapswapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab#修改内核参数cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1EOFsysctl -p#kubernetes 1.8+要求关闭swap分区(master节点，建议node节点也关闭swap)swapoff -a &amp;&amp; sysctl -w vm.swappiness=0#创建k8s组件安装目录mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl&#125;#配置selinuxsed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/configsetenforce 0#安装依赖包yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker官方yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo#安装docker-ceyum install docker-ce#启动dockersystemctl start docker#配置docker加速和私有仓库cat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"192.168.0.210:5000\"] #私有registry的地址&#125;EOF#删除上面文件中的\"私有registry注释\"sed -i 's/#.*//g' /etc/docker/daemon.json#重启docker服务systemctl restart docker 5.1 准备自签名证书(其中一台机器操作即可) 组件 使用到的证书 etcd ca.pem, kubernetes-key.pem, kubernetes.pem flannel ca.pem, kubernetes-key.pem, kubernetes.pem kube-apiserver ca.pem, kubernetes-key.pem, kubernetes.pem kubelet ca.pem kube-proxy ca.pem, kube-proxy.pem, kube-proxy-key.pem kubectl ca.pem, admin.pem, admin-key.pem kube-controller 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 kube-schedule 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 123456789101112131415#安装证书生成个工具wget -c https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl*mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo#创建一个证书的目录sslmkdir sslcd ssl#生成证书模板cfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json 5.2 修改刚生成的json文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110根据config.json创建ca-config.json:&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;ca-csr.json:#需要注意的是CN,O和OU的属性都不要修改&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;kubernetes-csr.json:&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"etcd101\", \"etcd102\", \"etcd103\", \"127.0.0.1\", \"172.17.8.101\", \"172.17.8.102\", \"172.17.8.103\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;admin-csr.json:&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;kube-proxy-csr.json&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; 5.3 生成相应的证书和key1234567891011#生成ca的证书和keycfssl gencert -initca ca-csr.json | cfssljson -bare ca#生成服务器证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes#生成admin的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin#生成kube-proxy的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 5.4 查看一下是否生成了相应的证书和key123456789101112#仅保留相应的pem证书mkdir ../certmv *.pem ../cert#校验证书cfssl-certinfo -cert kubernetes.pem#查看相应的证书是否生成[root@k8s101 ssl]# cd ../cert/[root@k8s101 cert]# lsadmin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem[root@k8s101 cert]# 6.1 安装etcd组件(所有机器)12345678#下载etcdwget -c https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz#解压安装tar xf etcd-v3.1.12-linux-amd64.tar.gzcd etcd-v3.1.12-linux-amd64mv etcd* /opt/kubernetes/bin/mkdir /var/lib/etcd 6.2 分别配置每个机器的参数(k8s101为例)1234567891011121314cat &gt; /opt/kubernetes/cfg/etcd &lt;&lt; EOF#[Member]ETCD_NAME=\"etcd101\"ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd/\"ETCD_LISTEN_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.17.8.101:2379,http://127.0.0.1:2379\"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.17.8.101:2379\"ETCD_INITIAL_CLUSTER=\"etcd101=https://172.17.8.101:2380,etcd102=https://172.17.8.102:2380,etcd103=https://172.17.8.103:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF 6.3 配置etcd的服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#把下面这个粘贴到/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetwants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/opt/kubernetes/cfg/etcd#User=etcd# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) \\/opt/kubernetes/bin/etcd \\--name=\\\"$&#123;ETCD_NAME&#125;\\\" \\--data-dir=\\\"$&#123;ETCD_DATA_DIR&#125;\\\" \\--listen-peer-urls=\\\"$&#123;ETCD_LISTEN_PEER_URLS&#125;\\\" \\--listen-client-urls=\\\"$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\\" \\--advertise-client-urls=\\\"$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\\\" \\--initial-advertise-peer-urls=\\\"$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125;\\\" \\--initial-cluster=\\\"$&#123;ETCD_INITIAL_CLUSTER&#125;\\\" \\--initial-cluster-token=\\\"$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\\\" \\--initial-cluster-state=\\\"$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\\\" \\--cert-file=/opt/kubernetes/ssl/kubernetes.pem\\--key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--peer-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\--peer-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\" \\--client-cert-auth=\"\\true\\\" \\--peer-client-cert-auth=\\\"true\\\" \\--auto-tls=\\\"true\\\" \\--peer-auto-tls=\\\"true\\\"\"Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target#使得service生效systemctl daemon-reload#把需要用到的证书拷贝到目的地cp ca.pem /opt/kubernetes/ssl/cp server.pem /opt/kubernetes/ssl/cp server-key.pem /opt/kubernetes/ssl/#启动服务systemctl start etcd#验证服务/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 cluster-health#也可以这样验证#[root@k8s103 ssl]# curl -s --cacert /opt/kubernetes/ssl/ca.pem https://172.17.8.101:2379/v2/members | jq .[root@k8s103 ssl]# curl -s http://127.0.0.1:2379/v2/members |jq .&#123; \"members\": [ &#123; \"id\": \"52a549447e771f7\", \"name\": \"etcd102\", \"peerURLs\": [ \"https://172.17.8.102:2380\" ], \"clientURLs\": [ \"https://172.17.8.102:2379\" ] &#125;, &#123; \"id\": \"680948716edebf39\", \"name\": \"etcd103\", \"peerURLs\": [ \"https://172.17.8.103:2380\" ], \"clientURLs\": [ \"https://172.17.8.103:2379\" ] &#125;, &#123; \"id\": \"bf65263f1f4624b4\", \"name\": \"etcd101\", \"peerURLs\": [ \"https://172.17.8.101:2380\" ], \"clientURLs\": [ \"https://172.17.8.101:2379\" ] &#125; ]&#125; 7. 下载flannel并安装应用到所有Node节点,假设k8s102123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#下载flannel-v0.10wget -c https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz#解压tar xf flannel-v0.10.0-linux-amd64.tar.gz#添加flannel的配置文件/opt/kubernetes/cfg/flanneldFLANNEL_OPTIONS=\"--etcd-endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 -etcd-cafile=/opt/kubernetes/ssl/ca.pem -etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"#创建一个供flanneld工作的目录mkdir -p /opt/kubernetes/run/flanneld/#把下面这段粘贴到/usr/lib/systemd/system/flanneld.service[Unit]Description=Flannel overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld -iface=eth1 --ip-masq $FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /opt/kubernetes/run/flanneld/subnet.envRestart=on-failure[Install]wantedBy=multi-user.target#第一个节点启动flanneld之前需要先设置vxlan网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 set /coreos.com/network/config '&#123;\"Network\": \"172.20.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'#尽量使用172开头的网段#查看网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 get /coreos.com/network/config#查看生成的网段信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 ls /coreos.com/network/subnets#修改docker的服务，#在[Service]中ExecStart上面添加一行为EnvironmentFile=/opt/kubernetes/run/flanneld/subnet.env#下面的ExecStart修改为ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS#重启docker服务systemctl daemon-reloadsystemctl restart docker#此时，每个节点分配的docker0的网段应该都是不一样的，ping一下另外一个节点docker0的地址测试 8.1 生成k8s TLS Boostrapping Token(在任意一个master上操作)12345mkdir kubeconfig &amp;&amp; cd kubeconfigexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 8.2 创建kubelet bootstrapping kubeconfig文件123456789101112131415161718192021222324252627282930313233#安装kubectl客户端工具wget -c https://dl.k8s.io/v1.10.5/kubernetes-client-linux-amd64.tar.gztar xf kubernetes-client-linux-amd64.tar.gzcd kubernetes/clientmv kubectl /opt/kubernetes/bin/export KUBE_APISERVER=\"https://172.17.8.101:6443\"#添加环境变量vi ~/.bash_profilePATH修改为 PATH=$PATH:$HOME/bin:/opt/kubernetes/binsource ~/.bash_profile#创建kubectl bootstrapping kubeconfig文件# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 8.3 创建kube-proxy bootstrapping kubeconfig文件123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 9.把刚才创建的kubeconfig文件拷贝到所有的node节点上，位置先随意1234scp kube-proxy.kubeconfig node1_ip:~/...scp bootstrap.kubeconfig node1_ip:~/... 10. 安装master节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158rm -rf kuberneteswget -c https://dl.k8s.io/v1.10.5/kubernetes-server-linux-amd64.tar.gztar xf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bin#把相应的二进制文件放到指定位置mv kube-apiserver /opt/kubernetes/bin/mv kube-controller-manager kube-scheduler /opt/kubernetes/bin/#把下面的内容保存为apiserver.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"192.168.1.195\"&#125;ETCD_SERVERS=$&#123;2:-\"http://127.0.0.1:2379\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--etcd-servers=$&#123;ETCD_SERVERS&#125; \\\\--insecure-bind-address=127.0.0.1 \\\\--bind-address=$&#123;MASTER_ADDRESS&#125; \\\\--insecure-port=8080 \\\\--secure-port=6443 \\\\--advertise-address=$&#123;MASTER_ADDRESS&#125; \\\\--allow-privileged=true \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\\\--kubelet-https=true \\\\--enable-bootstrap-token-auth \\\\--token-auth-file=/opt/kubernetes/cfg/token.csv \\\\--service-node-port-range=30000-50000 \\\\--tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\\\--tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\\\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--etcd-cafile=/opt/kubernetes/ssl/ca.pem \\\\--etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\\\--etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-apiserversystemctl restart kube-apiserver#执行apiserver.sh./apiserver.sh 172.17.8.101 https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379#拷贝刚才生成的tokencd ~/kubeconfigcp token.csv /opt/kubernetes/cfg/#启动kube-apiserversystemctl start kube-apiserver#把下面的内容保存为controller-manager.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect=true \\\\--address=127.0.0.1 \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--cluster-name=kubernetes \\\\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--root-ca-file=/opt/kubernetes/ssl/ca.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-controller-managersystemctl restart kube-controller-manager#安装和启动controller-manager./controller-manager.sh 127.0.0.1#验证controller-manager是否启动systemctl status kube-controller-manager#把下面的内容保存为scheduler.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-schedulersystemctl restart kube-scheduler#安装并启动kube-scheduler./scheduler.sh 127.0.0.1#查看是否启动systemctl status kube-scheduler#验证服务是否都正常kubectl get cs#输入如下NAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-1 Healthy &#123;\"health\": \"true\"&#125;#至此master配置完毕kubectl get svc#如果要修改网段的话，可以修改api-server和controller-manager相应的配置文件，重启服务然后删除kubernetes服务即可#kubectl delete svc default 11. 安装配置node 建议kubelet参数修改加上KUBELET_EXTRA_ARGS= “–fail-swap-on=false” 忽略swap的配置 kubelet v1.11.1 建议配置 KUBE_PROXY_MODE=ipvs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112#在master上执行，创建角色绑定kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap#下面都是在node节点上操作#把刚才传过来的kubeconfig文件都放在指定位置cd kubeconfig/cp *kubeconfig /opt/kubernetes/cfg/#下载client包wget -c https://dl.k8s.io/v1.10.5/kubernetes-node-linux-amd64.tar.gztar xf kubernetes-node-linux-amd64.tar.gzcd kubernetes/node/bin/mv kubelet kube-proxy /opt/kubernetes/bin/#把下面的内容保存为kubelet.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.196\"&#125;DNS_SERVER_IP=$&#123;2:-\"10.10.10.2\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kubeletKUBELET_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--address=$&#123;NODE_ADDRESS&#125; \\\\--hostname-override=$&#123;NODE_ADDRESS&#125; \\\\--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\\--experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\\--cert-dir=/opt/kubernetes/ssl \\\\--allow-privileged=true \\\\--cluster-dns=$&#123;DNS_SERVER_IP&#125; \\\\--cluster-domain=cluster.local \\\\--fail-swap-on=false \\\\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/kubeletExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet#执行脚本bash kubelet.sh 172.17.8.102 10.254.0.2#把下面的内容保存到proxy.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.200\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=$&#123;NODE_ADDRESS&#125; \\--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-proxysystemctl restart kube-proxy#执行proxy.shbash proxy.sh 172.17.8.102#然后在master节点上操作,看到是Pending状态kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 2m kubelet-bootstrap Pending#这时候在master上授权允许就可以了kubectl certificate approve node-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4#再次查看状态就变过来了kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 4m kubelet-bootstrap Approved,Issued#查看节点kubectl get nodesNAME STATUS ROLES AGE VERSION172.17.8.102 Ready &lt;none&gt; 1m v1.10.5#其他的客户端跟这个一样操作即可，等node节点状态变为Ready就可用了#测试一个实例kubectl run nginx --image=nginx --replicas=3kubectl scale --replicas=4 deployment/nginxkubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort 12. 配置一个客户端kubectl123456789101112131415#把admin.pem和admin-key.pem,ca.pem拷贝到你想配置的节点上#在这个节点上下载kubectlkubectl config set-cluster kubernetes --server=https://172.17.8.101:6443 --certificate-authority=ca.pemkubectl config set-credentials cluster-admin --certificate-authority=ca.pem --client-key=admin-key.pem --client-certificate=admin.pemkubectl config set-context default --cluster=kubernetes --user=cluster-adminkubectl config use-context default#测试kubectl get cskubectl get csr#其实会自动生成配置文件，路径为 ~/.kube/config 13.1 配置Dashboard相关yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#把下面内容保存为dashboard-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kubernetes-dashboard-minimal namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system#把下面内容保存为dashboard-deployment.yamlapiVersion: apps/v1beta2kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: kubernetes-dashboard containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.7.1 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 100Mi ports: - containerPort: 9090 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\"#把下面的内容保存为dashboard-service.yamlapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090 13.2 安装dashboard12345678910111213141516kubectl create -f dashboard-rbac.yamlkubectl create -f dashboard-deployment.yamlkubectl create -f dashboard-service.yaml#查看并测试kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.254.116.162 &lt;none&gt; 80:31523/TCP 1d#看到了31523了吧，执行下面的命令kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkubernetes-dashboard-b9f5f9d87-jszl5 1/1 Running 0 1d 172.68.24.2 172.17.8.102#看到了dashboard在172.17.8.102这个node上了吧#访问http://172.17.8.102:31523 就可以打开这个dashboard 14.1 把下面内容保存为coredns.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156apiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/16 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\"spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" containers: - name: coredns image: index.tenxcloud.com/xiangyu123/coredns:1.1.3 imagePullPolicy: IfNotPresent args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\"spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP 14.2 安装过程1kubectl create -f coredns.yaml","categories":[{"name":"容器云","slug":"容器云","permalink":"http://xiangyu123.github.io/categories/容器云/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xiangyu123.github.io/tags/k8s/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://xiangyu123.github.io/tags/kubernetes/"}]}]}