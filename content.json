{"meta":{"title":"Tobey's note","subtitle":null,"description":"Tobey's personal blog","author":"Tobey","url":"http://xiangyu123.github.io"},"pages":[{"title":"about","date":"2018-10-17T10:49:03.000Z","updated":"2018-10-17T10:52:54.530Z","comments":true,"path":"about/index.html","permalink":"http://xiangyu123.github.io/about/index.html","excerpt":"","text":"关于我 Tobey, 多年搬砖经验，哎…就这么多"}],"posts":[{"title":"虚拟化技术和kvm实践","slug":"kvm","date":"2018-11-28T08:29:51.000Z","updated":"2018-11-28T09:31:51.167Z","comments":true,"path":"2018/11/28/kvm/","link":"","permalink":"http://xiangyu123.github.io/2018/11/28/kvm/","excerpt":"","text":"虚拟化技术与KVM1. kvm与xen的比较 对比项 XEN KVM 问世时间 2003 2007 操作系统支持 UNIX、Linux和Microsoft Windows UNIX、Linux和Microsoft Windows 支持的虚拟化技术 全虚拟化、半虚拟化 全虚拟化 动态迁移 支持 支持(基于共享存储) 内核要求 需要对内核打补丁 内置在内核中 新版升级方式 需要编译内核 无需编译内核，仅需要加载最新的ko模块 性能 接近宿主机 稍弱一点 管理工具对比 QEMU-XEN、xm、virsh QEMU-KVM、Libvirt、virsh、virt-manager 2.kvm支持的image格式 vmdk(vmware) vhdx(Hyper-V) vdi(VirtualBox) rbd(ceph) raw qcow2 3.虚拟机格式的兼容性 虚拟机 raw qcow2 vmdk vdi vhd XEN √ √ √ √ KVM √ √ √ √ VMware √ 4.qcow2和raw文件格式的对比 raw格式由于裸的彻底，性能上来说的话还是不错的。目前来看，KVM和XEN默认的格式好像还是这个格式。因为其原始，有很多原生的特性，例如直接挂载也是一件简单的事情。 裸的好处还有就是简单，支持转换成其它格式的虚拟机镜像对裸露的它来说还是很简单的（如果其它格式需要转换，有时候还是需要它做为中间格式），空间使用来看，这个很像磁盘，使用多少就是多少（du -h看到的大小就是使用大小），但如果你要把整块磁盘都拿走的话得全盘拿了（copy镜像的时候），会比较消耗网络带宽和I/O qcow2现在比较主流的一种虚拟化镜像格式，经过一代的优化，目前qcow2的性能上接近raw裸格式的性能，且具有以下优点: 更小的存储空间，即使是不支持holes的文件系统也可以（这下du -h和ls -lh看到的就一样了） Copy-on-write support, where the image only represents changes made to an underlying disk image（这个特性SUN ZFS表现的淋漓尽致） 支持多个snapshot，对历史snapshot进行管理 支持zlib的磁盘压缩 支持AES的加密Notes: 建议使用qcow2格式 5. kvm安装(基于centos7.5)1[root@k8s201 ~]# yum -y install qemu-kvm-tools qemu-kvm.x86_64 libvirt-daemon-kvm.x86_64 virt-install 6. 启动kvm12[root@k8s201 ~]# systemctl start libvirtd[root@k8s201 ~]# systemctl enable libvirtd 7. 创建image硬盘文件1[root@k8s201 ~]# qemu-img create -f qcow2 -o size=20Gb kvm_disk1.img 8. 查看支持的os-variant1[root@k8s201 ~]# osinfo-query os 9.安装并配置openvswitch12345678910111213141516171819202122232425262728293031323334353637383940# 安装openvswitch[root@k8s201 ~]# yum install openvswitch#固定ip[root@k8s201 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ovsbr0DEVICE=ovsbr0ONBOOT=yesDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=192.168.100.1NETMASK=255.255.255.0HOTPLUG=noZONE=trusted# 定义ovs网络类型，保持为ovsbr0.xml&lt;network&gt; &lt;name&gt;ovsbr0&lt;/name&gt; &lt;forward mode='bridge'/&gt; &lt;bridge name='ovsbr0'/&gt; &lt;virtualport type='openvswitch'/&gt;&lt;/network&gt;#注册网络类型到kvm[root@k8s201 ~]# virsh net-destroy default[root@k8s201 ~]# virsh net-autostart default --disable[root@k8s201 ~]# virsh net-define ovsbr0.xml[root@k8s201 ~]# virsh net-start ovsbr0[root@k8s201 ~]# virsh net-autostart ovsbr0[root@k8s201 ~]# virsh net-list Name State Autostart Persistent---------------------------------------------------------- ovsbr0 active yes yes# 设置NAT[root@k8s201 ~]# sysctl -w net.ipv4.ip_forward=1[root@k8s201 ~]# echo \"net.ipv4.ip_forward=1\" &gt;&gt; /etc/sysctl.conf[root@k8s201 ~]# firewall-cmd --zone=trusted --add-interface=ovsbr0[root@k8s201 ~]# firewall-cmd --zone=public --add-masquerade[root@k8s201 ~]# firewall-cmd --zone=public --add-masquerade --permanent 9. 安装Guest1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 ########### 使用使用iso来安装 ########### # virt-install \\ --name=centos5 \\ --os-variant=RHEL5 \\ --ram=512 \\ --vcpus=1 \\ --disk path=/opt/vms/centos63-webtest.img,format=qcow2,size=7,bus=virtio \\ --accelerate \\ --cdrom /data/iso/CentOS5.iso \\ --vnc --vncport=5910 \\ --vnclisten=0.0.0.0 \\ --network bridge=br0,model=virtio \\ --noautoconsole ######### 从http中启动，使用ks安装 ######### # virt-install \\ --name=centos63-webtest \\ --os-variant=RHEL6 \\ --ram=4096 \\ --vcpus=4 \\ --virt-type kvm \\ --disk path=/opt/vms/centos63-webtest.img,format=qcow2,size=7,bus=virtio \\ --accelerate \\ --location http://192.168.130.4/centos63 \\ --extra-args \"linux ip=192.168.73.22 netmask=255.255.255.224 gateway=192.168.73.1 ks=http://192.168.130.4/ks/xen63.ks\"\\ --vnc --vncport=5910 \\ --vnclisten=0.0.0.0 \\ --network bridge=br0,model=virtio \\ --force \\ --noautoconsole ########## 从http安装，使用ks, 双网卡, 启用console ######## # virt-install \\ --name=centos63-webtest \\ --os-variant=RHEL6 \\ --ram=4096 \\ --vcpus=4 \\ --virt-type kvm \\ --disk path=/opt/vms/centos63-webtest.img,format=qcow2,size=7,bus=virtio \\ --accelerate \\ --location http://111.205.130.4/centos63 \\ --extra-args \"linux ip=59.151.73.22 netmask=255.255.255.224 gateway=59.151.73.1 ks=http://111.205.130.4/ks/xen63.ks console=ttyS0 serial\" --vnc --vncport=5910 --vnclisten=0.0.0.0 \\ --network bridge=br0,model=virtio \\ --network bridge=br1,model=virtio \\ --force \\ --noautoconsole --nographics --extra-args=\"text console=tty0 console=ttyS0,115200\" 9. 虚拟机相关操作命令1234567891011121314151617181920[root@k8s201 ~]# qemu-img info kvm_disk1.img #查看磁盘格式[root@k8s201 ~]# virsh list --all #查看所有虚拟机状况[root@k8s201 ~]# virsh start guest1 #启动名为guest1的虚拟机[root@k8s201 ~]# virsh shutdown guest1 #关闭名为guest1的虚拟机[root@k8s201 ~]# virsh destroy guest1 #强制关机[root@k8s201 ~]# virsh create /etc/libvirt/qemu/wintest01.xml #通过已定义的xml文件创建虚拟机[root@k8s201 ~]# virsh autostart guest1 #设置自动启动[root@k8s201 ~]# virsh dumpxml wintest01 #导出虚拟机配置[root@k8s201 ~]# virsh undefine wintest01 #删除虚拟机配置，destroy后彻底删除虚拟机[root@k8s201 ~]# virsh define /etc/libvirt/qemu/wintest01.xml #重新定义虚拟机[root@k8s201 ~]# virsh edit wintest01 #编辑虚拟机配置文件[root@k8s201 ~]# virsh suspend wintest01 #挂起虚拟机[root@k8s201 ~]# virsh resume wintest01 #恢复挂起的虚拟机[root@k8s201 ~]# virt-clone -o centos63_webtest -n centos63_webtest2 -f /opt/vms/centos_webtest2.img #克隆虚拟机, 使用原虚拟机centos63_webtest克隆为centos63_webtest2,且磁盘位置放在/opt/vms/centos_webtest2.img[root@k8s201 ~]# qemu-img convert -f raw -O qcow2 centos63-119.22.raw centos63-119.22.img #转换格式[root@k8s201 ~]# virsh snapshot-create centos63-119 #为这个虚拟机创建快照[root@k8s201 ~]# qemu-img info centos63-119 #查看虚拟机快照信息[root@k8s201 ~]# virsh snapshot-revert centos63-119.22 1410341560 #恢复快照，需要在虚拟机关机状况下[root@k8s201 ~]# virsh snapshot-delete centos63-119 #删除快照 10. 添加虚拟机网卡123[root@k8s201 ~]# virsh attach-interface centos63-119 --type bridge --source br1 --model virtio[root@k8s201 ~]# cd /etc/libvirt/qemu[root@k8s201 qemu]# virsh dumpxml centos63-119 &gt; centos63-119.xml 11. 硬盘扩容 分区是lvm格式 这种很简单，添加一块磁盘，lvm扩容 1234567[root@k8s201 ~]# virt-img create -f qcow2 10G.img 10G[root@k8s201 ~]# virsh attach-disk centos63-119 10G.img vdb[root@k8s201 ~]# cd /etc/libvirt/qemu[root@k8s201 qemu]# virsh dumpxml centos63-119 &gt; centos63-119.xml# 接下来就是走LVM扩容的流程了# 参考 http://www.cnblogs.com/cmsd/p/3964118.html 分区不是lvm格式，该分区不是扩展分区, 需要关机离线扩展 123456789101112131415161718192021222324252627282930313233343536# 新建一个磁盘，大于原来的容量，比如原来是40G，你想对某个分区扩容20G那么[root@k8s201 ~]# qemu-img create -f qcow2 60G.img 60G# 备份原来的磁盘，以防三长两短[root@k8s201 ~]# cp centos63-119.img centos63-119.img.bak# 查看原来的磁盘决定扩容哪一个分区[root@k8s201 ~]# yum -y install libguestfs-tools[root@k8s201 ~]# virt-filesystems --partitions --long -a centos63-119.img[root@k8s201 ~]# virt-df centos63-119.img# 扩容GuestOS的sda2[root@k8s201 ~]# virt-resize --expand /dev/sda2 centos63-119.27.img 60G.img# 使用新磁盘启动[root@k8s201 ~]# mv 60G.img centos63-119.img[root@k8s201 ~]# virsh start centos63-119# 在线迁移(需要使用共享存储)[root@k8s201 ~]# virsh migrate centos63-119.27 --live qemu+ssh://192.168.119.11:9741/system –unsafe# 删除虚拟机[root@k8s201 ~]# virsh destroy clone1[root@k8s201 ~]# virsh undefine clone1[root@k8s201 ~]# rm -f /data/clone1.img# 虚拟机停机迁移[root@k8s202 ~]# virsh shutdown test02[root@k8s202 ~]# virsh dumpxml test02 &gt; test02.xml[root@k8s202 ~]# virsh domblklist test02 #查看磁盘位置[root@k8s202 ~]# rsync -av /data/test02.qcow2 server02:/data/test02.qcow2[root@k8s202 ~]# rsync -av test02.xml server02:test02.xml[root@k8s202 ~]# #在server02上操作[root@k8s202 ~]# virsh define test02.xml[root@k8s202 ~]# virsh start test02","categories":[],"tags":[{"name":"kvm","slug":"kvm","permalink":"http://xiangyu123.github.io/tags/kvm/"}]},{"title":"golang接口型函数详解","slug":"golang-interface-function","date":"2018-11-14T03:20:09.000Z","updated":"2018-11-14T03:36:53.224Z","comments":true,"path":"2018/11/14/golang-interface-function/","link":"","permalink":"http://xiangyu123.github.io/2018/11/14/golang-interface-function/","excerpt":"","text":"实例11234567891011121314151617181920212223242526272829303132333435363738394041package mainimport ( \"fmt\")type Handler interface &#123; Do(k, v interface&#123;&#125;)&#125;type HandlerFunc func(k, v interface&#123;&#125;)func (f HandlerFunc) Do(k, v interface&#123;&#125;) &#123; f(k, v)&#125;func Each(m map[interface&#123;&#125;]interface&#123;&#125;, h Handler) &#123; if m != nil &amp;&amp; len(m) &gt; 0 &#123; for k, v := range m &#123; h.Do(k, v) &#125; &#125;&#125;func EachFunc(m map[interface&#123;&#125;]interface&#123;&#125;, f func(k, v interface&#123;&#125;)) &#123; Each(m, HandlerFunc(f))&#125;func selfInfo(k, v interface&#123;&#125;) &#123; fmt.Printf(\"大家好,我叫%s,今年%d岁\\n\", k, v)&#125;func main() &#123; persons := make(map[interface&#123;&#125;]interface&#123;&#125;) persons[\"张三\"] = 20 persons[\"李四\"] = 23 persons[\"王五\"] = 26 EachFunc(persons, selfInfo)&#125; 实例21234567891011121314151617181920212223package mainimport \"fmt\"type Back interface &#123; backcall(int)&#125;type wwjdbfun func(int) intfunc (w wwjdbfun) backcall(n int) &#123; fmt.Println(\"backcall:\", n) w(n + 1)&#125;func runfun(n int) int &#123; fmt.Println(\"wwjdbfun run1:\", n) return 1&#125;func main() &#123; w := wwjdbfun(runfun) w.backcall(10)&#125; 转载自: Golang必备技巧：接口型函数","categories":[{"name":"编程","slug":"编程","permalink":"http://xiangyu123.github.io/categories/编程/"}],"tags":[{"name":"golang","slug":"golang","permalink":"http://xiangyu123.github.io/tags/golang/"},{"name":"go","slug":"go","permalink":"http://xiangyu123.github.io/tags/go/"}]},{"title":"openvswitch简介","slug":"openvswitch-intro","date":"2018-10-19T15:48:27.000Z","updated":"2018-10-19T16:22:16.469Z","comments":true,"path":"2018/10/19/openvswitch-intro/","link":"","permalink":"http://xiangyu123.github.io/2018/10/19/openvswitch-intro/","excerpt":"","text":"Open Vswitch简介 什么是OpenvSwitch OpenvSwitch，简称OVS是一个虚拟交换软件，主要用于虚拟机VM环境，作为一个虚拟交换机，支持Xen/XenServer, KVM, and VirtualBox多种虚拟化技术。 整个OVS代码用C写的。目前有以下功能： Standard 802.1Q VLAN model with trunk and access ports NIC bonding with or without LACP on upstream switch NetFlow, sFlow(R), and mirroring for increased visibility QoS (Quality of Service) configuration, plus policing GRE, GRE over IPSEC, VXLAN, and LISP tunneling 802.1ag connectivity fault management OpenFlow 1.0 plus numerous extensions Transactional configuration database with C and Python bindings High-performance forwarding using a Linux kernel module IPv6 support STP support OpenvSwitch 组成 ovs-vswitchd：守护程序，实现交换功能，和Linux内核兼容模块一起，实现基于流的交换flow-based switching ovsdb-server：轻量级的数据库服务，主要保存了整个OVS的配置信息，包括接口啊，交换内容，VLAN啊等等。ovs-vswitchd会根据数据库中的配置信息工作 ovs-dpctl：一个工具，用来配置交换机内核模块，可以控制转发规则。 ovs-vsctl：主要是获取或者更改ovs-vswitchd的配置信息，此工具操作的时候会更新ovsdb-server中的数据库 ovs-appctl：主要是向OVS守护进程发送命令的，一般用不上 ovsdbmonitor：GUI工具来显示ovsdb-server中数据信息 ovs-controller：一个简单的OpenFlow控制器 ovs-ofctl：用来控制OVS作为OpenFlow交换机工作时候的流表内容 ovs结构图 ovsdb-server ovs数据处理流程 流表的匹配 收到数据包后，会交给datapath内核模块处理，当匹配到对应的datapath会直接输出，如果没有匹配到，会交给用户态的ovs-vswitchd查询flow，用户态处理后，会把处理完的数据包输出到正确的端口，并且设置新的datapath规则，后续数据包可以通过新的datapath规则实现快速转发。 ovs相关概念Packet 网络转发的最小数据单元，每个包都来自某个端口，最终会被发往一个或多个目标端口，转发数据包的过程就是网络的唯一功能。 Bridge Open vSwitch中的网桥对应物理交换机，其功能是根据一定流规则，把从端口收到的数据包转发到另一个或多个端口。 Port 端口是收发数据包的单元。Open vSwitch中，每个端口都属于一个特定的网桥。端口收到的数据包会经过流规则的处理，发往其他端口；也会把其他端口来的数据包发送出去。 Open vSwitch支持的端口有以下几种： Normal Port: 用户可以把操作系统中的网卡绑定到Open vSwitch上，Open vSwitch会生成一个普通端口处理这块网卡进出的数据包 Internal Port: 当设置端口类型为internal，Open vSwitch会创建一快虚拟网卡，此端口收到的所有数据包都会交给这块网卡，网卡发出的包会通过这个端口交给Open vSwitch。Note: 当Open vSwitch创建一个新网桥时，默认会创建一个与网桥同名的Internal Port Patch Port: 当机器中有多个Open vSwitch网桥时，可以使用Patch Port把两个网桥连起来。Patch Port总是成对出现，分别连接在两个网桥上，在两个网桥之间交换数据。 Interface 接口是Open vSwitch与外部交换数据包的组件。一个接口就是操作系统的一块网卡，这块网卡可能是Open vSwitch生成的虚拟网卡，也可能是物理网卡挂载在Open vSwitch上，也可能是操作系统的虚拟网卡（TUN/TAP）挂载在Open vSwitch上 Flow 流定义了端口之间数据包的交换规则。每条流分为匹配和动作两部分，匹配部分选择哪些数据包需要可以通过这条流处理，动作决定这些匹配到的数据包如何转发。流描述了一个网桥上，端口到端口的转发规则。比如我可以定义这样一条流: 当数据包来自端口A，则发往端口B 当数据包来自端口A，并且其源MAC是aa:aa:aa:aa:aa:aa，并且其拥有vlan tag为a，并且其源IP是a.a.a.a，并且其协议是TCP，其TCP源端口号为a，则修改其源IP为b.b.b.b，发往端口B Datapath Datapath是流的一个缓存，会把流的match结果cache起来，避免下一次流继续到用户空间进行flow match flow table 每个 datapath都和一个“flowtable”关联，当 datapath接收到数据之后， OVS会在 flow table中查找可以匹配的 flow，执行对应的操作,例如转发数据到另外的端口 网桥的工作原理网桥处理包遵循以下几条规则 在一个接口上接收到的包不会再往那个接口上发送此包 每个接收到的包都要学习其源MAC地址 如果数据包是多播或者广播包（通过2层MAC地址确定）则要向接收端口以外的所有端口转发，如果上层协议感兴趣，则还会递交上层处理 如果数据包的地址不能再CAM表中找到，则向接收端口以外的其他端口转发 如果CAM表中能找到，则转发给相应端口，如果发送和接收都是统一端口，则不发送 网桥是以混杂模式工作 相关操作 查看学习到的MAC地址 1ovs-appctl fdb/show br0 创建网桥 1ovs-vsctl add-br ovs-switch -- set Bridge br0 fail-mode=secure 创建端口p0并配置其OpenFlow端口编号为100 1ovs-vsctl add-port br0 p0 -- set Interface p0 ofport_request=$i 设置端口设备类型为internal 1ovs-vsctl set Interface p0 type=internal 查看创建的结果 1234567891011121314151617[root@k8s201 ~]# ovs-vsctl showaea2e0f0-9994-4703-9ff0-9ec12f3a1d50 Bridge &quot;br0&quot; fail_mode: secure Port &quot;p1&quot; Interface &quot;p1&quot; Port &quot;br0&quot; Interface &quot;br0&quot; type: internal Port &quot;p3&quot; Interface &quot;p3&quot; Port &quot;p2&quot; Interface &quot;p2&quot; Port &quot;p4&quot; Interface &quot;p4&quot; ovs_version: &quot;2.0.0&quot;[root@k8s201 ~]# 查看所有的 flow table 1ovs-ofctl dump-tables br0 查看流表 1234567891011121314151617181920212223[root@k8s201 ~]# ovs-ofctl dump-flows br0NXST_FLOW reply (xid=0x4): cookie=0x0, duration=7310.456s, table=0, n_packets=0, n_bytes=0, idle_age=7310, priority=0 actions=resubmit(,1) cookie=0x0, duration=7357.2s, table=0, n_packets=0, n_bytes=0, idle_age=7357, dl_src=01:00:00:00:00:00/01:00:00:00:00:00 actions=drop cookie=0x0, duration=7348.329s, table=0, n_packets=0, n_bytes=0, idle_age=7348, dl_dst=01:80:c2:00:00:00/ff:ff:ff:ff:ff:f0 actions=drop Notes: dump-flows字段解释 Cookie：流规则标识。 duration：流表项创建持续的时间（单位是秒）。 table：流表项所属的table编号。 n_packets：此流表项匹配到的报文数。 n_bytes：此流表项匹配到的字节数。 idle_age：此流表项从最后一个匹配的报文到现在空闲的时间。 hard_age：此流表项从最后一次被创建或修改到现在持续的时间。 Priority：流表项的优先级，数字越大优先级越高，范围是：0~7。 删除OpenFlow端口号为100的所有流表项 1ovs-ofctl del-flows br0 \"in_port=100\" 屏蔽广播包 1ovs-ofctl add-flow br0 \"table=0, dl_src=01:00:00:00:00:00/01:00:00:00:00:00, actions=drop\" 开启stp支持 1ovs-vsctl set bridge br0 stp_enable=true 关闭stp 1ovs-vsctl set bridge br0 stp_enable=false 查询stp 1ovs-vsctl get bridge br0 stp_enable 设置stp优先级 1ovs−vsctl set bridge br0 other_config:stp-priority=0x7800 设置stp Cost 1ovs−vsctl set port eth0 other_config:stp-path-cost=10 移除stp配置 1ovs−vsctl clear bridge br0 other_config 配置OpenFlow版本 1ovs-vsctl set bridge ovs-br protocols=OpenFlow12,OpenFlow13 移除openflow支持 1ovs-vsctl clear bridge br0 protocols 设置接口的vlan编号 1ovs-vsctl set port p0 tag=30 设置接口为trunk 1ovs-vsctl set port p0 vlan=trunk 设置trunk允许通过的vlan 1ovs-vsctl set port p1 trunk=4,5,6 设置网桥的faild-mode 1ovs-vsctl set-fail-mode ovs-br secure 调试一个数据包的规则 12345678910111213[root@k8s201 ~]# ovs-appctl ofproto/trace br0 in_port=1,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02Flow: metadata=0,in_port=1,vlan_tci=0x0000,dl_src=00:00:00:00:00:01,dl_dst=00:00:00:00:00:02,dl_type=0x0000Rule: table=0 cookie=0 priority=0OpenFlow actions=resubmit(,1) Resubmitted flow: unchanged Resubmitted regs: reg0=0x0 reg1=0x0 reg2=0x0 reg3=0x0 reg4=0x0 reg5=0x0 reg6=0x0 reg7=0x0 Resubmitted odp: drop No matchFinal flow: unchangedRelevant fields: skb_priority=0,in_port=1,dl_src=00:00:00:00:00:00/01:00:00:00:00:00,dl_dst=00:00:00:00:00:00/ff:ff:ff:ff:ff:f0,dl_type=0x0000,nw_frag=noDatapath actions: drop 查看模块的log配置 1ovs-appctl vlog/list 配置模块的日志级别 1ovs-appctl vlog/set dpif:console:dbg 添加flow规则(屏蔽某个ip) 1ovs-ofctl add-flow xenbr0 idle_timeout=0,dl_type=0x0800,nw_src=119.75.213.50,actions=drop 数据包重定向 1ovs-ofctl add-flow xenbr0 idle_timeout=0,dl_type=0x0800,nw_proto=1,actions=output:4 更改源ip转发 1ovs-ofctl add-flow xenbr0 idle_timeout=0,in_port=3,actions=mod_nw_src:211.68.52.32,normal 查看端口的统计信息 1ovs-ofctl dump-ports br0 查看有哪些网桥,哪些port,哪些interface 12345678910111213141516171819root@l-network-1:~# ovs-vsctl list-brbr-exbr-intbr-tunroot@l-network-1:~# ovs-vsctl list-ports br-tunpatch-intvxlan-ac1c0509vxlan-ac1c050dvxlan-ac1c051cvxlan-ac1c053froot@l-network-1:~# ovs-vsctl list-ifaces br-tunpatch-intvxlan-ac1c0509vxlan-ac1c050dvxlan-ac1c051cvxlan-ac1c053f# iface与ports同名. 查看某个port,interface属于哪个bridge 1234root@l-network-1:~# ovs-vsctl port-to-br vxlan-ac1c0509br-tunroot@l-network-1:~# ovs-vsctl iface-to-br vxlan-ac1c0509br-tun 流表常用字段 常见actions 参考: http://fishcried.com/2016-02-09/openvswitch-ops-guide/参考: https://opengers.github.io/openstack/openstack-base-openflow-in-openvswitch/参考: https://opengers.github.io/openstack/openstack-base-use-openvswitch/实验: https://vcpu.me/openvswitch/","categories":[],"tags":[{"name":"ovs","slug":"ovs","permalink":"http://xiangyu123.github.io/tags/ovs/"},{"name":"openvswitch","slug":"openvswitch","permalink":"http://xiangyu123.github.io/tags/openvswitch/"},{"name":"sdn","slug":"sdn","permalink":"http://xiangyu123.github.io/tags/sdn/"}]},{"title":"ldap和kerberos安装","slug":"ldap_kerberos_install","date":"2018-10-17T10:22:22.000Z","updated":"2018-10-17T10:42:20.839Z","comments":true,"path":"2018/10/17/ldap_kerberos_install/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/ldap_kerberos_install/","excerpt":"","text":"OpenLdap搭建 LDAP是一款轻量级目录访问协议（Lightweight Directory Access Protocol，简称LDAP），属于开源集中账号管理架构的实现 1、OpenLdap简介&nbsp;&nbsp;&nbsp;&nbsp;OpenLDAP目录中的信息是以树状的层次结构来存储数据(（这很类同于DNS），最顶 层即根部称作“基准DN”，形如“dc=mydomain,dc=org”或者“o=mydomain.org”，前一种方式更为灵活也是Windows AD中使用的方式。 &nbsp;&nbsp;&nbsp;&nbsp;在根目录的下面有很多的文件和目录，为了把这些大量的数据从逻辑上分开，OpenLDAP像其它的目录服务协议一样使用OU（Organization Unit，组织单元），可以用来表示公司内部机构，如部门等，也可以用来表示设备、人员等。同时OU还可以有子OU，用来表示更为细致的分类。 &nbsp;&nbsp;&nbsp;&nbsp;OpenLDAP中每一条记录都有一个唯一的区别于其它记录的名字DN（Distinguished Name）,其处在“叶子”位置的部分称作RDN(用户条目的相对标识名)。如dn:cn=tom,ou=animals,dc=ilanni,dc=com中cn即为RDN，而RDN在一个OU中必须是唯一的。 其中o=dlw.com是dc=dlw,dc=com的简写 部署的组织架构:12345dn: dc=qbos,dc=com # 公司dn: ou=krb5,dc=qbos,dc=com # 部门dn: cn=admin,dc=qbos,dc=com # LDAP管理用户dn: cn=kdc-srv,ou=krb5,dc=qbos,dc=com # kdc连接ldap用户dn: cn=adm-srv,ou=krb5,dc=qbos,dc=com # kadmin连接ldap用户 1、安装软件包1# yum install openldap openldap-servers openldap-clients openldap-devel compat-openldap -y 2、初始化配置12# cp /usr/share/openldap-servers/slapd.conf.obsolete /etc/openldap/slapd.conf# cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG 生产ldap服务管理用户密码12# slappasswd -s 123123&#123;SSHA&#125;FGTQwm900Ac+FPPTpzB9R1ZHNVY/MOXP # hash加密后的值，直接复制到配置文件中使用 修改slap.conf，支持kerberos123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# vim /etc/openldap/slapd.confinclude /etc/openldap/schema/corba.schemainclude /etc/openldap/schema/core.schemainclude /etc/openldap/schema/cosine.schemainclude /etc/openldap/schema/nis.schema# kerberos使用的schema文件include /etc/openldap/schema/kerberos.schemapidfile /var/run/openldap/slapd.pidargsfile /var/run/openldap/slapd.argsdatabase bdb# 组织后缀suffix &quot;dc=qbos,dc=com&quot; # 管理员账号rootdn &quot;cn=admin,dc=qbos,dc=com&quot;# 管理员密码(slappasswd生产的密码)rootpw &#123;SSHA&#125;FslGWaaPsw9XcM7Sr3gc24lFQQe1NxRA directory /var/lib/ldap# ldap数据库索引字段index krbPrincipalName eq,pres,sub index objectClass eq,presloglevel 256# acl规则access to attrs=userPassword,shadowLastChange by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by anonymous auth by self write by * noneaccess to dn.subtree=&quot;ou=krb5,dc=qbos,dc=com&quot; by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write # kerberos服务连接ladp的账号 by dn=&quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot; write # kerberos服务连接ladp的账号 by dn=&quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot; read by * noneaccess to attrs=loginShell by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by self write by * noneaccess to dn.base=&quot;&quot; by * readaccess to * by self write by * readaccess to * by dn=&quot;cn=admin,dc=qbos,dc=com&quot; write by users read by * noneaccess to * by dn.exact=&quot;cn=admin,dc=qbos,dc=com&quot; read by * none 3、初始化ldap配置&nbsp;&nbsp;&nbsp;&nbsp;官方 对于 OpenLDAP 2.4 ，不推荐使用 slapd.conf 作为配置文件。从这个版本开始所有配置数据都保存在 /etc/openldap/slapd.d/中,在slapd.d生成配置文件，但是还是能使用slapd.conf修改配置的，每次修改后都要执行slaptest -f 命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# rm -rf /etc/openldap/slapd.d/*修改文件权限# chown ldap.ldap -R /var/lib/ldap/每次修改完sldp.conf配置文件都要删除重新执行# slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d/报错忽略&gt; 5b99e55c bdb_db_open: database &quot;dc=qbos,dc=com&quot;: db_open(/var/lib/ldap/id2entry.bdb) failed: No such file or directory (2).&gt; 5b99e55c backend_startup_one (type=bdb, suffix=&quot;dc=qbos,dc=com&quot;): bi_db_open failed! (2)&gt; slap_startup failed (test would succeed using the -u switch)# chown ldap.ldap -R /etc/openldap/slapd.d/启动sldap服务# /etc/init.d/slapd start初始化ldap库，编写kerberos.ldif文件# vim /tmp/kerberos.ldifdn: dc=qbos, dc=comdc: qbosobjectClass: topobjectClass: dcObjectobjectClass: organizationo: qbos.comdn: ou=krb5, dc=qbos,dc=comou: krb5objectClass: organizationalUnitdn: cn=kdc-srv,ou=krb5,dc=qbos,dc=comcn: kdc-srvobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: &#123;SSHA&#125;o4pwTcWIoY/CX8mx7gUyy0PRdwfKilAx dn: cn=adm-srv,ou=krb5,dc=qbos,dc=comcn: adm-srvobjectClass: simpleSecurityObjectobjectClass: organizationalRoleuserPassword: &#123;SSHA&#125;o4pwTcWIoY/CX8mx7gUyy0PRdwfKilAx注：userPassword中的密码是slappasswd生成的导入kerberos.ldif到ldap库中# ldapadd -x -D &apos;cn=admin,dc=qbos,dc=com&apos; -w 123123 -h 127.0.0.1 -f /tmp/kerberos.ldif注：userPassword中的密码是slappasswd生成的, -w 是&apos;cn=admin,dc=qbos,dc=com&apos;真实的用户密码查看ldap数据库信息# ldapsearch -x -D &apos;cn=admin,dc=qbos,dc=com&apos; -w 123123 -b &apos;cn=adm-srv,ou=krb5,dc=qbos,dc=com&apos; 搭建kerberos服务端 Kerberos 服务是单点登录系统，这意味着您对于每个会话只需向服务进行一次自我验证，即可自动保护该会话过程中所有后续事务的安全。服务对您进行验证后，即无需在每次使用基于 Kerberos 的服务时进行验证 官方文档 1、安装过程1# yum install krb5-server* krb5-auth-dialog* krb5-libs* krb5-workstation* -y 2、修改配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162编辑krb5.conf# vim /etc/krb5.conf[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = QBOS.COM # realm要与ldap DC保持一致 dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h # 票据生命周期 renew_lifetime = 7d forwardable = false[realms] QBOS.COM = &#123; kdc = 127.0.0.1:88 # kdc 服务 kadmin_server = 127.0.0.1:749 # 远程管理kdc服务 database_module = openldap_ldapconf # 数据信息保存 &#125;[domain_realm] .qbos.com = QBOS.COM qbos.com = QBOS.COM[kdc] profile = /var/kerberos/krb5kdc/kdc.conf # 密钥分发中心配置文件[dbdefaults] ldap_kerberos_container_dn = ou=krb5,dc=qbos,dc=com[dbmodules] openldap_ldapconf = &#123; db_library = kldap ldap_kdc_dn = &quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot; # kdc-srv访问ldap用户 ldap_kadmind_dn = &quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot; # adm-srv kadmin访问ldap用户 ldap_service_password_file = /var/kerberos/krb5kdc/service.keyfile # 用户密码文件,使用kdb5_ldap_util生成 ldap_servers = ldap://127.0.0.1 # ldap服务 ldap_conns_per_server = 5&#125;# mkdir /var/kerberos/krb5kdc/ssl/# vim /var/kerberos/krb5kdc/kdc.conf[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] QBOS.COM = &#123; pkinit_identity = FILE:/var/kerberos/krb5kdc/ssl/kdc.pem,/var/kerberos/krb5kdc/ssl/kdckey.pem # kdc服务X.509格式的证书 pkinit_anchors = FILE:/var/kerberos/krb5kdc/ssl/cacert.pem # 指定KDC信任签署客户端证书的可信根证书(CA)的位置 acl_file = /var/kerberos/krb5kdc/kadm5.acl # 文件应包含所有获许管理 KDC 的主体名称 dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab # kadmind主体的Keytab ：kadmin / fqdn，changepw / fqdn和kadmin / changepw supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal &#125; 配置文件详解: https://docs.oracle.com/cd/E86824_01/html/E54775/kdc.conf-4.html 如果使用pkinit配置，生产principal时要添加 +requires_preauth 向数据库添加管理主体可以根据需要添加任意数目的 admin 主体。至少必须添加一个 admin 主体，这样才能完成 KDC 配置过程1234# kadmin.local: addprinc kadmin/adminEnter password for principal kadmin/admin@QBOS.COM:&lt;Type the password&gt;Re-enter password for principal kadmin/admin@QBOS.COM: &lt;Type it again&gt;Principal &quot;kadmin/admin@QBOS.COM&quot; created. 为 kadmind 服务创建一个密钥表文件此命令序列创建一个包含 kadmin/ 和 changepw/ 的主体项的特殊密钥表文件。kadmind 服务需要使用这些主体，要更改口令也需要使用这些主体1234# kadmin.local: ktadd -k /var/kerberos/krb5kdc/kadm5.keytab kadmin/admin@QBOS.COM# kadmin.local: ktadd -k /var/kerberos/krb5kdc/kadm5.keytab kadmin/changepw@QBOS.COM参考文件：https://docs.oracle.com/cd/E24847_01/html/819-7061/setup-9.html 3、生成访问ldap的服务密码文件1234# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -w 123123 stashsrvpw -f /var/kerberos/krb5kdc/service.keyfile &quot;cn=kdc-srv,ou=krb5,dc=qbos,dc=com&quot;# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -w 123123 stashsrvpw -f /var/kerberos/krb5kdc/service.keyfile &quot;cn=adm-srv,ou=krb5,dc=qbos,dc=com&quot;注：cn=admin,dc=qbos,dc=com 为ldap管理员与密码 4、创建kerberos数据库123# kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -H ldap:// create -r QBOS.COM 注：输入cn=admin,dc=qbos,dc=com密码以及创建KDC database master key密码 5、启动kerberos服务123456789# /etc/init.d/krb5kdc start# /etc/init.d/kadmin start# netstat -tnlp|egrep &apos;kadmin|krb5kdc&apos;tcp 0 0 0.0.0.0:749 0.0.0.0:* LISTEN 11829/kadmind tcp 0 0 0.0.0.0:464 0.0.0.0:* LISTEN 11829/kadmind tcp 0 0 0.0.0.0:88 0.0.0.0:* LISTEN 11454/krb5kdc tcp 0 0 :::749 :::* LISTEN 11829/kadmind tcp 0 0 :::464 :::* LISTEN 11829/kadmind tcp 0 0 :::88 :::* LISTEN 11454/krb5kdc 6、测试创建kerberos用户1234567891011121314151617181920212223242526# kadmin.local&gt; Authenticating as principal root/admin@QBOS.COM with password.&gt; kadmin.local: addprinc admintest&gt; WARNING: no policy specified for admintest@QBOS.COM; defaulting to no policy&gt; Enter password for principal &quot;admintest@QBOS.COM&quot;: &gt; Re-enter password for principal &quot;admintest@QBOS.COM&quot;: &gt; Principal &quot;admintest@QBOS.COM&quot; created.&gt; kadmin.local: q查看创建的用户是否写入ldap库中# slapcat |grep admintest5b9a34c3 bdb_monitor_db_open: monitoring disabled; configure monitor database to enabledn: krbPrincipalName=admintest@QBOS.COM,cn=QBOS.COM,ou=krb5,dc=qbos,dc=comkrbPrincipalName: admintest@QBOS.COM验证用户登录kinit admintestPassword for admintest@QBOS.COM: # klist Ticket cache: FILE:/tmp/krb5cc_0Default principal: admintest@QBOS.COMValid starting Expires Service principal09/13/18 17:58:46 09/14/18 17:58:46 krbtgt/QBOS.COM@QBOS.COM renew until 09/13/18 17:58:46 7、问题12345krb5kdc: Can not fetch master key (error: ?????????). - while fetching master key K/M for realm QBOS.COM原因：未初始化kerberos数据库解决：kdb5_ldap_util -D &quot;cn=admin,dc=qbos,dc=com&quot; -H ldap:// create -r QBOS.COM (数据库为LDAP)如果不是LDAP，使用 /usr/sbin/kdb5_util create -s 搭建kerberos客户端1、安装kerberos客户端1# yum install -y krb5-libs krb5-workstation krb5-devel 2、修改配置文件123456789101112131415161718192021222324252627282930313233修改kerberos配置文件# vim /etc/krb5.conf[libdefaults] default_realm = QBOS.COM #默认域 dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = false[realms] QBOS.COM = &#123; kdc = kdc1.qbos.com:88 # kdc配置 kdc = kdc2.qbos.com:88 kdc = kdc3.qbos.com:88 admin_server = kadmin.qbos.com:749 # kadmin配置 &#125;[domain_realm] .qbos.com = QBOS.COM qbos.com = QBOS.COM 修改sshd_conf,使其支持kerberos认证# vim /etc/ssh/sshd_config添加GSSAPIAuthentication yesGSSAPICleanupCredentials yesKerberosAuthentication yesKerberosOrLocalPasswd yesKerberosTicketCleanup yes# /etc/init.d/sshd restart 3、生成keytab密钥表 &nbsp;&nbsp;&nbsp;&nbsp;keytab密钥表是可以免密登录，如果用户有独立的密钥表是可以免密登录,提供服务的每台主机都必须包含称为 keytab（密钥表）的本地文件。 &nbsp;&nbsp;&nbsp;&nbsp; 要将服务密钥添加至密钥表文件，应使用 kadmin 的 ktadd 命令，将相应的服务主体添加至主机的密钥表文件。由于要将服务主体添加至密钥表文件，因此该主体必须已存在于 Kerberos 数据库中，以便 kadmin 可验证其存在创建主机密钥表 12345678创建主机kerberos主体# kadmin.local -p kadmin/admin -q &quot;addprinc -policy host_policy -randkey host/$HOSTNAME@QBOS.COM&quot;创建主机keytab密钥表# kadmin.local -p kadmin/admin -q &quot;ktadd -k /root/krb5.keytab.$IP host/$HOSTNAME@QBOS.COM&quot;将主机的keytab密钥表拷贝到客户端/etc/# scp /root/krb5.keytab.$IP $HOSTNAME:/etc/ 4、kerberos用户授权主机用户配置文件： .k5login - 用于主机访问的Kerberos V5 acl文件，该文件驻留在用户的主目录，包含了Kerberos主体列表12345在客户端主机172.28.0.106的root家目录下修改.k5login，授权给用户admintest# vim ~/.k5login...admintest@QBOS.COM admintest通过kinit获取到票据后就可以ssh root@172.28.0.106 注：-policy为指定密码安全策略，可以使用 list_policies 查看密码策略列表，使用get_policy查看策略详情 kerberos双活与主从kerberos 双主+主从实际上是通知ldap实现的，只需实现LDAP的双主+主从 1、LDAP双主+主从部署LDAP双主配置文档LDAP主从配置文档 2、kerberos服务端配置按照 搭建kerberos客户端 安装kerberos客户端，将 另一台 kerberos服务端配置CA、key、.k5.QBOS.COM 拷贝至相应位置即可 参考文档：https://www.ilanni.com/?p=13775https://www.cnblogs.com/zhaojonjon/p/5967281.htmlhttps://www.cnblogs.com/zihanxing/p/7001201.htmlhttps://docs.oracle.com/cd/E26926_01/html/E25889/refer-15.htmlhttp://k5wiki.kerberos.org/wiki/LDAP_on_Kerberos kerberos主从https://help.ubuntu.com/lts/serverguide/kerberos-ldap.html.en kerberos x.509证书https://web.mit.edu/kerberos/krb5-latest/doc/admin/pkinit.html ssh conf配置https://www.cnblogs.com/Rozdy/p/4642928.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xiangyu123.github.io/categories/Linux/"}],"tags":[{"name":"ldap","slug":"ldap","permalink":"http://xiangyu123.github.io/tags/ldap/"}]},{"title":"802.1x(EAP-PEAP | EAP-TLS)认证过程","slug":"wifi-authoriz","date":"2018-10-17T10:22:22.000Z","updated":"2018-12-06T09:02:25.064Z","comments":true,"path":"2018/10/17/wifi-authoriz/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/wifi-authoriz/","excerpt":"","text":"802.1x认证过程分析认证过程详见如下图: 过滤器 (eapol &amp;&amp; eth.addr == 80:e6:50:21:90:b0 &amp;&amp; eth.addr == 80:f6:2e:53:79:00) or bootp 客户端抓包命令 tcpdump -G 3600 -n (ether proto 0x888e or (port 67 or port 68) or icmp) -w %Y%m%d_%H%M.cap 客户端(STA)主动发起EAPOL(eap over lan)的start报文(#355)，告诉authenticator(ac),开始认证 authenticator使用EAP协议发送Request,Identify报文(#356)，要求STA发送用户名(仅要求发送用户名,密码在后面发送) STA针对authenticator的Request,Identify报文(#356)回复Response,Identify的报文(#356)，用户名为xuzg，如图 authenticator以EAP Over RADIUS的报文格式将EAP-Response/Identity发送给认证服务器Radius,并且带上相关的RADIUS的属性(#11403) Radius收到客户端发来的EAP-Response/Identity，根据配置确定使用EAP-PEAP认证，并向AP发送RADIUS- Access-Challenge报文，里面含有Radius发送给客户端的EAP-Request/Peap/Start的报文，表示希望开始进行 EAP-PEAP的认证(#11404)","categories":[{"name":"wireless","slug":"wireless","permalink":"http://xiangyu123.github.io/categories/wireless/"}],"tags":[{"name":"network","slug":"network","permalink":"http://xiangyu123.github.io/tags/network/"}]},{"title":"k8s-install","slug":"k8s-install","date":"2018-10-17T03:34:27.000Z","updated":"2018-10-17T10:25:15.000Z","comments":true,"path":"2018/10/17/k8s-install/","link":"","permalink":"http://xiangyu123.github.io/2018/10/17/k8s-install/","excerpt":"","text":"1. 环境准备 机器名 资源配置 操作系统 角色 IP k8s101 2/cpu+2G/mem CentOS7.4-x86_64 Master 172.17.8.101 k8s102 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.102 k8s103 2/cpu+2G/mem CentOS7.4-x86_64 Node 172.17.8.103 2. 应用版本准备 docker版本 etcd版本 CoreDNS版本 kubernetes版本 flannel版本 docker-ce-18.03.1.ce-1.el7 etcd-v3.1.12 v1.1.3 v1.10.5 v0.10.0 3. 机器角色的分配 k8s101- kube-apiserver - kube-controller-manager - kube-scheduler - etcd k8s102- kubelet - kube-proxy - docker - flannel - etcd k8s103- kubelet - kube-proxy - docker - flannel - etcd 4. 所有机器安装集群依赖的软件包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#清空防火墙规则iptables -t nat -F &amp;&amp; iptables -t nat -X &amp;&amp; iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -Z &amp;&amp; iptables -t nat -Z#k8s 1.8+要求关闭swapswapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab#修改内核参数cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1EOFsysctl -p#kubernetes 1.8+要求关闭swap分区(master节点，建议node节点也关闭swap)swapoff -a &amp;&amp; sysctl -w vm.swappiness=0#创建k8s组件安装目录mkdir -p /opt/kubernetes/&#123;bin,cfg,ssl&#125;#配置selinuxsed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/configsetenforce 0#安装依赖包yum -y install yum-utils device-mapper-persistent-data lvm2#添加docker官方yum源yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo#安装docker-ceyum install docker-ce#启动dockersystemctl start docker#配置docker加速和私有仓库cat &lt;&lt; EOF &gt; /etc/docker/daemon.json&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"192.168.0.210:5000\"] #私有registry的地址&#125;EOF#删除上面文件中的\"私有registry注释\"sed -i 's/#.*//g' /etc/docker/daemon.json#重启docker服务systemctl restart docker 5.1 准备自签名证书(其中一台机器操作即可) 组件 使用到的证书 etcd ca.pem, kubernetes-key.pem, kubernetes.pem flannel ca.pem, kubernetes-key.pem, kubernetes.pem kube-apiserver ca.pem, kubernetes-key.pem, kubernetes.pem kubelet ca.pem kube-proxy ca.pem, kube-proxy.pem, kube-proxy-key.pem kubectl ca.pem, admin.pem, admin-key.pem kube-controller 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 kube-schedule 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书 123456789101112131415#安装证书生成个工具wget -c https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget -c https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl*mv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonmv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo#创建一个证书的目录sslmkdir sslcd ssl#生成证书模板cfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.json 5.2 修改刚生成的json文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110根据config.json创建ca-config.json:&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;ca-csr.json:#需要注意的是CN,O和OU的属性都不要修改&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;kubernetes-csr.json:&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"etcd101\", \"etcd102\", \"etcd103\", \"127.0.0.1\", \"172.17.8.101\", \"172.17.8.102\", \"172.17.8.103\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;admin-csr.json:&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;kube-proxy-csr.json&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; 5.3 生成相应的证书和key1234567891011#生成ca的证书和keycfssl gencert -initca ca-csr.json | cfssljson -bare ca#生成服务器证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes#生成admin的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin#生成kube-proxy的证书和keycfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 5.4 查看一下是否生成了相应的证书和key123456789101112#仅保留相应的pem证书mkdir ../certmv *.pem ../cert#校验证书cfssl-certinfo -cert kubernetes.pem#查看相应的证书是否生成[root@k8s101 ssl]# cd ../cert/[root@k8s101 cert]# lsadmin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem server-key.pem server.pem[root@k8s101 cert]# 6.1 安装etcd组件(所有机器)12345678#下载etcdwget -c https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz#解压安装tar xf etcd-v3.1.12-linux-amd64.tar.gzcd etcd-v3.1.12-linux-amd64mv etcd* /opt/kubernetes/bin/mkdir /var/lib/etcd 6.2 分别配置每个机器的参数(k8s101为例)1234567891011121314cat &gt; /opt/kubernetes/cfg/etcd &lt;&lt; EOF#[Member]ETCD_NAME=\"etcd101\"ETCD_DATA_DIR=\"/var/lib/etcd/default.etcd/\"ETCD_LISTEN_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.17.8.101:2379,http://127.0.0.1:2379\"#[Clustering]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.17.8.101:2380\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.17.8.101:2379\"ETCD_INITIAL_CLUSTER=\"etcd101=https://172.17.8.101:2380,etcd102=https://172.17.8.102:2380,etcd103=https://172.17.8.103:2380\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_INITIAL_CLUSTER_STATE=\"new\"EOF 6.3 配置etcd的服务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#把下面这个粘贴到/usr/lib/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetwants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/opt/kubernetes/cfg/etcd#User=etcd# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) \\/opt/kubernetes/bin/etcd \\--name=\\\"$&#123;ETCD_NAME&#125;\\\" \\--data-dir=\\\"$&#123;ETCD_DATA_DIR&#125;\\\" \\--listen-peer-urls=\\\"$&#123;ETCD_LISTEN_PEER_URLS&#125;\\\" \\--listen-client-urls=\\\"$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\\" \\--advertise-client-urls=\\\"$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125;\\\" \\--initial-advertise-peer-urls=\\\"$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125;\\\" \\--initial-cluster=\\\"$&#123;ETCD_INITIAL_CLUSTER&#125;\\\" \\--initial-cluster-token=\\\"$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125;\\\" \\--initial-cluster-state=\\\"$&#123;ETCD_INITIAL_CLUSTER_STATE&#125;\\\" \\--cert-file=/opt/kubernetes/ssl/kubernetes.pem\\--key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--peer-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\--peer-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\" \\--client-cert-auth=\"\\true\\\" \\--peer-client-cert-auth=\\\"true\\\" \\--auto-tls=\\\"true\\\" \\--peer-auto-tls=\\\"true\\\"\"Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target#使得service生效systemctl daemon-reload#把需要用到的证书拷贝到目的地cp ca.pem /opt/kubernetes/ssl/cp server.pem /opt/kubernetes/ssl/cp server-key.pem /opt/kubernetes/ssl/#启动服务systemctl start etcd#验证服务/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 cluster-health#也可以这样验证#[root@k8s103 ssl]# curl -s --cacert /opt/kubernetes/ssl/ca.pem https://172.17.8.101:2379/v2/members | jq .[root@k8s103 ssl]# curl -s http://127.0.0.1:2379/v2/members |jq .&#123; \"members\": [ &#123; \"id\": \"52a549447e771f7\", \"name\": \"etcd102\", \"peerURLs\": [ \"https://172.17.8.102:2380\" ], \"clientURLs\": [ \"https://172.17.8.102:2379\" ] &#125;, &#123; \"id\": \"680948716edebf39\", \"name\": \"etcd103\", \"peerURLs\": [ \"https://172.17.8.103:2380\" ], \"clientURLs\": [ \"https://172.17.8.103:2379\" ] &#125;, &#123; \"id\": \"bf65263f1f4624b4\", \"name\": \"etcd101\", \"peerURLs\": [ \"https://172.17.8.101:2380\" ], \"clientURLs\": [ \"https://172.17.8.101:2379\" ] &#125; ]&#125; 7. 下载flannel并安装应用到所有Node节点,假设k8s102123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#下载flannel-v0.10wget -c https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz#解压tar xf flannel-v0.10.0-linux-amd64.tar.gz#添加flannel的配置文件/opt/kubernetes/cfg/flanneldFLANNEL_OPTIONS=\"--etcd-endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 -etcd-cafile=/opt/kubernetes/ssl/ca.pem -etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"#创建一个供flanneld工作的目录mkdir -p /opt/kubernetes/run/flanneld/#把下面这段粘贴到/usr/lib/systemd/system/flanneld.service[Unit]Description=Flannel overlay address etcd agentAfter=network-online.target network.targetBefore=docker.service[Service]type=notifyEnvironmentFile=/opt/kubernetes/cfg/flanneldExecStart=/opt/kubernetes/bin/flanneld -iface=eth1 --ip-masq $FLANNEL_OPTIONSExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /opt/kubernetes/run/flanneld/subnet.envRestart=on-failure[Install]wantedBy=multi-user.target#第一个节点启动flanneld之前需要先设置vxlan网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 set /coreos.com/network/config '&#123;\"Network\": \"172.20.0.0/16\", \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'#尽量使用172开头的网段#查看网络信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 get /coreos.com/network/config#查看生成的网段信息/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/kubernetes.pem --key-file=/opt/kubernetes/ssl/kubernetes-key.pem --endpoints=https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379 ls /coreos.com/network/subnets#修改docker的服务，#在[Service]中ExecStart上面添加一行为EnvironmentFile=/opt/kubernetes/run/flanneld/subnet.env#下面的ExecStart修改为ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS#重启docker服务systemctl daemon-reloadsystemctl restart docker#此时，每个节点分配的docker0的网段应该都是不一样的，ping一下另外一个节点docker0的地址测试 8.1 生成k8s TLS Boostrapping Token(在任意一个master上操作)12345mkdir kubeconfig &amp;&amp; cd kubeconfigexport BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 8.2 创建kubelet bootstrapping kubeconfig文件123456789101112131415161718192021222324252627282930313233#安装kubectl客户端工具wget -c https://dl.k8s.io/v1.10.5/kubernetes-client-linux-amd64.tar.gztar xf kubernetes-client-linux-amd64.tar.gzcd kubernetes/clientmv kubectl /opt/kubernetes/bin/export KUBE_APISERVER=\"https://172.17.8.101:6443\"#添加环境变量vi ~/.bash_profilePATH修改为 PATH=$PATH:$HOME/bin:/opt/kubernetes/binsource ~/.bash_profile#创建kubectl bootstrapping kubeconfig文件# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 8.3 创建kube-proxy bootstrapping kubeconfig文件123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=/opt/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \\ --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem \\ --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 9.把刚才创建的kubeconfig文件拷贝到所有的node节点上，位置先随意1234scp kube-proxy.kubeconfig node1_ip:~/...scp bootstrap.kubeconfig node1_ip:~/... 10. 安装master节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158rm -rf kuberneteswget -c https://dl.k8s.io/v1.10.5/kubernetes-server-linux-amd64.tar.gztar xf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bin#把相应的二进制文件放到指定位置mv kube-apiserver /opt/kubernetes/bin/mv kube-controller-manager kube-scheduler /opt/kubernetes/bin/#把下面的内容保存为apiserver.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"192.168.1.195\"&#125;ETCD_SERVERS=$&#123;2:-\"http://127.0.0.1:2379\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--etcd-servers=$&#123;ETCD_SERVERS&#125; \\\\--insecure-bind-address=127.0.0.1 \\\\--bind-address=$&#123;MASTER_ADDRESS&#125; \\\\--insecure-port=8080 \\\\--secure-port=6443 \\\\--advertise-address=$&#123;MASTER_ADDRESS&#125; \\\\--allow-privileged=true \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\\\--kubelet-https=true \\\\--enable-bootstrap-token-auth \\\\--token-auth-file=/opt/kubernetes/cfg/token.csv \\\\--service-node-port-range=30000-50000 \\\\--tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \\\\--tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \\\\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\\\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--etcd-cafile=/opt/kubernetes/ssl/ca.pem \\\\--etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \\\\--etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \\$KUBE_APISERVER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-apiserversystemctl restart kube-apiserver#执行apiserver.sh./apiserver.sh 172.17.8.101 https://172.17.8.101:2379,https://172.17.8.102:2379,https://172.17.8.103:2379#拷贝刚才生成的tokencd ~/kubeconfigcp token.csv /opt/kubernetes/cfg/#启动kube-apiserversystemctl start kube-apiserver#把下面的内容保存为controller-manager.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect=true \\\\--address=127.0.0.1 \\\\--service-cluster-ip-range=10.254.0.0/16 \\\\--cluster-name=kubernetes \\\\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\\\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\\\--root-ca-file=/opt/kubernetes/ssl/ca.pem\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-controller-managersystemctl restart kube-controller-manager#安装和启动controller-manager./controller-manager.sh 127.0.0.1#验证controller-manager是否启动systemctl status kube-controller-manager#把下面的内容保存为scheduler.sh#!/bin/bashMASTER_ADDRESS=$&#123;1:-\"127.0.0.1\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--master=$&#123;MASTER_ADDRESS&#125;:8080 \\\\--leader-elect\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \\$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-schedulersystemctl restart kube-scheduler#安装并启动kube-scheduler./scheduler.sh 127.0.0.1#查看是否启动systemctl status kube-scheduler#验证服务是否都正常kubectl get cs#输入如下NAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Healthy &#123;\"health\": \"true\"&#125;etcd-2 Healthy &#123;\"health\": \"true\"&#125;etcd-1 Healthy &#123;\"health\": \"true\"&#125;#至此master配置完毕kubectl get svc#如果要修改网段的话，可以修改api-server和controller-manager相应的配置文件，重启服务然后删除kubernetes服务即可#kubectl delete svc default 11. 安装配置node 建议kubelet参数修改加上KUBELET_EXTRA_ARGS= “–fail-swap-on=false” 忽略swap的配置 kubelet v1.11.1 建议配置 KUBE_PROXY_MODE=ipvs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112#在master上执行，创建角色绑定kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap#下面都是在node节点上操作#把刚才传过来的kubeconfig文件都放在指定位置cd kubeconfig/cp *kubeconfig /opt/kubernetes/cfg/#下载client包wget -c https://dl.k8s.io/v1.10.5/kubernetes-node-linux-amd64.tar.gztar xf kubernetes-node-linux-amd64.tar.gzcd kubernetes/node/bin/mv kubelet kube-proxy /opt/kubernetes/bin/#把下面的内容保存为kubelet.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.196\"&#125;DNS_SERVER_IP=$&#123;2:-\"10.10.10.2\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kubeletKUBELET_OPTS=\"--logtostderr=true \\\\--v=4 \\\\--address=$&#123;NODE_ADDRESS&#125; \\\\--hostname-override=$&#123;NODE_ADDRESS&#125; \\\\--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \\\\--experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \\\\--cert-dir=/opt/kubernetes/ssl \\\\--allow-privileged=true \\\\--cluster-dns=$&#123;DNS_SERVER_IP&#125; \\\\--cluster-domain=cluster.local \\\\--fail-swap-on=false \\\\--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kubelet.service[Unit]Description=Kubernetes KubeletAfter=docker.serviceRequires=docker.service[Service]EnvironmentFile=-/opt/kubernetes/cfg/kubeletExecStart=/opt/kubernetes/bin/kubelet \\$KUBELET_OPTSRestart=on-failureKillMode=process[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kubeletsystemctl restart kubelet#执行脚本bash kubelet.sh 172.17.8.102 10.254.0.2#把下面的内容保存到proxy.sh#!/bin/bashNODE_ADDRESS=$&#123;1:-\"192.168.1.200\"&#125;cat &lt;&lt;EOF &gt;/opt/kubernetes/cfg/kube-proxyKUBE_PROXY_OPTS=\"--logtostderr=true \\--v=4 \\--hostname-override=$&#123;NODE_ADDRESS&#125; \\--kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig\"EOFcat &lt;&lt;EOF &gt;/usr/lib/systemd/system/kube-proxy.service[Unit]Description=Kubernetes ProxyAfter=network.target[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-proxyExecStart=/opt/kubernetes/bin/kube-proxy \\$KUBE_PROXY_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable kube-proxysystemctl restart kube-proxy#执行proxy.shbash proxy.sh 172.17.8.102#然后在master节点上操作,看到是Pending状态kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 2m kubelet-bootstrap Pending#这时候在master上授权允许就可以了kubectl certificate approve node-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4#再次查看状态就变过来了kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-6oXgQziElgXRb1eF0Q986YHP8tmmVcJVka1PD8Ox0l4 4m kubelet-bootstrap Approved,Issued#查看节点kubectl get nodesNAME STATUS ROLES AGE VERSION172.17.8.102 Ready &lt;none&gt; 1m v1.10.5#其他的客户端跟这个一样操作即可，等node节点状态变为Ready就可用了#测试一个实例kubectl run nginx --image=nginx --replicas=3kubectl scale --replicas=4 deployment/nginxkubectl expose deployment nginx --port=88 --target-port=80 --type=NodePort 12. 配置一个客户端kubectl123456789101112131415#把admin.pem和admin-key.pem,ca.pem拷贝到你想配置的节点上#在这个节点上下载kubectlkubectl config set-cluster kubernetes --server=https://172.17.8.101:6443 --certificate-authority=ca.pemkubectl config set-credentials cluster-admin --certificate-authority=ca.pem --client-key=admin-key.pem --client-certificate=admin.pemkubectl config set-context default --cluster=kubernetes --user=cluster-adminkubectl config use-context default#测试kubectl get cskubectl get csr#其实会自动生成配置文件，路径为 ~/.kube/config 13.1 配置Dashboard相关yaml文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#把下面内容保存为dashboard-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: Reconcile name: kubernetes-dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: kubernetes-dashboard-minimal namespace: kube-system labels: k8s-app: kubernetes-dashboard addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system#把下面内容保存为dashboard-deployment.yamlapiVersion: apps/v1beta2kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: kubernetes-dashboard containers: - name: kubernetes-dashboard image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.7.1 resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 100Mi ports: - containerPort: 9090 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\"#把下面的内容保存为dashboard-service.yamlapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090 13.2 安装dashboard12345678910111213141516kubectl create -f dashboard-rbac.yamlkubectl create -f dashboard-deployment.yamlkubectl create -f dashboard-service.yaml#查看并测试kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.254.116.162 &lt;none&gt; 80:31523/TCP 1d#看到了31523了吧，执行下面的命令kubectl get pod -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODEkubernetes-dashboard-b9f5f9d87-jszl5 1/1 Running 0 1d 172.68.24.2 172.17.8.102#看到了dashboard在172.17.8.102这个node上了吧#访问http://172.17.8.102:31523 就可以打开这个dashboard 14.1 把下面内容保存为coredns.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156apiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors health kubernetes cluster.local 10.254.0.0/16 &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\"spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" containers: - name: coredns image: index.tenxcloud.com/xiangyu123/coredns:1.1.3 imagePullPolicy: IfNotPresent args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\"spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP 14.2 安装过程1kubectl create -f coredns.yaml","categories":[{"name":"容器云","slug":"容器云","permalink":"http://xiangyu123.github.io/categories/容器云/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xiangyu123.github.io/tags/k8s/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://xiangyu123.github.io/tags/kubernetes/"}]}]}